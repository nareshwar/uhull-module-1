{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOBAL SUPERSTORE DATASET SALES ANALYSIS\n",
    "By Raju Vaneshwar Nareshwar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Task 1  <a id='task1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load first 10 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install packages\n",
    "!{sys.executable} -m pip install geopandas\n",
    "\n",
    "# import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "\n",
    "from matplotlib import ticker as mtick\n",
    "\n",
    "# Letting pandas to show max columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV file and assigning into a dataframe ss_data\n",
    "global_super_store_data = pd.read_csv('sample-superstore-2023-T3.csv')\n",
    "\n",
    "# Set the head to 10 to retrieve the first 10 records\n",
    "first_10_rows = global_super_store_data.head(n=10)\n",
    "first_10_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using info() and describe() function to get the descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metadata information about the dataset\n",
    "global_super_store_data.info()\n",
    "\n",
    "# Get descriptive statistics on the dataset\n",
    "global_super_store_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary key of these records are a system-generated, and denoted as column: *RowID*\n",
    "\n",
    "The datatypes of the dataset are following:\n",
    "* int64(1)\n",
    "* float64(2)\n",
    "* object(18)\n",
    "\n",
    "A few records of *Quantity* and *Profit* columns has the datatype of object, but it must be float64, thus needs to be cleansed or transformed.  \n",
    "*Ship Date* and *Order Date* columns are represented as strings, those needs to be converted as datetime.\n",
    "\n",
    "Once cleansed, the descriptive statistics can be applied to the numerial columns, and they are Sales, Quantity, Discount and Profit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function **text2float()** will take a txt number as a parameter and convert back to float64 number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2float(textnum, numwords={}):\n",
    "    try:\n",
    "        # Attempt to convert to float\n",
    "        return float(textnum)\n",
    "    except ValueError:\n",
    "        # If conversion to float fails, continue with text to number conversion\n",
    "        textnum = textnum.lower()\n",
    "        \n",
    "        if not numwords:\n",
    "            units = [\n",
    "                \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
    "                \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "                \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\",\n",
    "            ]\n",
    "\n",
    "            tens = [\"\", \"\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"]\n",
    "\n",
    "            scales = [\"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n",
    "\n",
    "            numwords[\"and\"] = (1, 0)\n",
    "            for idx, word in enumerate(units):\n",
    "                numwords[word] = (1, idx)\n",
    "            for idx, word in enumerate(tens):\n",
    "                numwords[word] = (1, idx * 10)\n",
    "            for idx, word in enumerate(scales):\n",
    "                numwords[word] = (10 ** (idx * 3 or 2), 0)\n",
    "\n",
    "        current = result = 0\n",
    "        for word in textnum.split():\n",
    "            if word not in numwords:\n",
    "                raise Exception(\"Illegal word: \" + word)\n",
    "\n",
    "            scale, increment = numwords[word]\n",
    "            current = current * scale + increment\n",
    "            if scale > 100:\n",
    "                result += current\n",
    "                current = 0\n",
    "\n",
    "        return result + current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_from_postal_code(postal_code):\n",
    "    if postal_code == '':\n",
    "        return None\n",
    "\n",
    "    url = f\"http://api.zippopotam.us/us/{postal_code}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        city = data['places'][0]['place name']\n",
    "        return city\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_from_postal_code(postal_code):\n",
    "    if postal_code == '':\n",
    "        return None\n",
    "\n",
    "    url = f\"http://api.zippopotam.us/us/{postal_code}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        state = data['places'][0]['state']\n",
    "        return state\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing values in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing any statistical analysis, the numerical column data has to be cleansed to be meaningful.\n",
    "* Records with special characters on *Quantity* and needs to be cleansed. \n",
    "* Records with special characters on *Profit* and needs to be cleansed. \n",
    "* Applying the **text2float()** function to fix *Quantity* column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row ID is not needed for the analysis, hence dropping the column\n",
    "if 'Row ID' in global_super_store_data.columns:\n",
    "    global_super_store_data.drop('Row ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing \"?\" from Quantity column\n",
    "global_super_store_data['Quantity'] = global_super_store_data['Quantity'].str.replace('?', '')\n",
    "\n",
    "# Removing \"\"\" from Profit column\n",
    "global_super_store_data['Profit'] = global_super_store_data['Profit'].str.replace('\"', '')\n",
    "\n",
    "# Assuming zero values for NaN on Profits\n",
    "global_super_store_data['Profit'] = global_super_store_data['Profit'].fillna(0)\n",
    "\n",
    "# Removing \"\"\" from Postal Code column\n",
    "global_super_store_data['Postal Code'] = global_super_store_data['Postal Code'].str.replace('\"', '')\n",
    "\n",
    "# Make all records as Country = United States\n",
    "global_super_store_data['Country'] = 'United States'\n",
    "\n",
    "# Correcting spelling mistakes on Category column\n",
    "global_super_store_data['Category'] = global_super_store_data['Category'].replace('Frnture', 'Furniture')\n",
    "\n",
    "# Filling values on empty Category/Sub-Category records\n",
    "global_super_store_data['Category'] = global_super_store_data['Category'].fillna('NO_CATEGORY')\n",
    "global_super_store_data['Sub-Category'] = global_super_store_data['Sub-Category'].fillna('NO_SUB_CATEGORY')\n",
    "\n",
    "# Datafix on Category based on subcategories\n",
    "# Apply the condition element-wise\n",
    "condition = (global_super_store_data['Category'] == 'NO_CATEGORY') & \\\n",
    "            (global_super_store_data['Sub-Category'].isin(['Binders', 'Storage']))\n",
    "\n",
    "# Update 'Category' where the condition is True\n",
    "global_super_store_data.loc[condition, 'Category'] = 'Office Supplies'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanse the Regions\n",
    "central_regions_to_replace = ['Centrl', 'Cntral']\n",
    "east_regions_to_replace = ['Est']\n",
    "south_regions_to_replace = ['Southh']\n",
    "\n",
    "global_super_store_data['Region'] = global_super_store_data['Region'].replace(central_regions_to_replace, 'Central')\n",
    "global_super_store_data['Region'] = global_super_store_data['Region'].replace(east_regions_to_replace, 'East')\n",
    "global_super_store_data['Region'] = global_super_store_data['Region'].replace(south_regions_to_replace, 'South')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying get_state_from_postal_code function\n",
    "\n",
    "# Filter the empty state rows\n",
    "state_filtered_na = global_super_store_data.loc[pd.isna(global_super_store_data['State'])]\n",
    "state_filtered_na\n",
    "\n",
    "# Apply the function to fill the missing value via API\n",
    "global_super_store_data.loc[pd.isna(global_super_store_data['State']), 'State'] = state_filtered_na['Postal Code'].apply(get_state_from_postal_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying get_city_from_postal_code function\n",
    "\n",
    "# Filter the empty city rows\n",
    "city_filtered_na = global_super_store_data.loc[pd.isna(global_super_store_data['City'])]\n",
    "city_filtered_na\n",
    "\n",
    "# Apply the function to fill the missing value via API\n",
    "global_super_store_data.loc[pd.isna(global_super_store_data['City']), 'City'] = city_filtered_na['Postal Code'].apply(get_city_from_postal_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying text2float function\n",
    "global_super_store_data['Quantity'] = global_super_store_data['Quantity'].apply(text2float)\n",
    "global_super_store_data['Profit'] = global_super_store_data['Profit'].apply(text2float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_super_store_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with missing data\n",
    "print(f\"Sum of null records:\\n{global_super_store_data.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping Sales/Profits based on Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_categories = global_super_store_data['Category'].unique()\n",
    "print(unique_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total sales by category from the highest sale.\n",
    "sales_category = global_super_store_data.groupby('Category')['Sales'].sum().sort_values(ascending=False)\n",
    "sales_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total profits by category\n",
    "profit_category = global_super_store_data.groupby('Category')['Profit'].sum().sort_values(ascending=False)\n",
    "profit_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total sales by category, only considering positive sales\n",
    "sales_based_on_category = global_super_store_data.groupby('Category').filter(lambda x: x['Sales'].sum() > 0).groupby('Category')['Sales'].sum()\n",
    "\n",
    "# group total profit by category, only considering positive profits\n",
    "profit_category = global_super_store_data.groupby('Category').filter(lambda x: x['Profit'].sum() > 0).groupby('Category')['Profit'].sum()\n",
    "\n",
    "# figure size\n",
    "plt.figure(figsize=(16,12));\n",
    "\n",
    "# left total sales pie chart\n",
    "plt.subplot(1,2,1); # 1 row, 2 columns, the 1st plot.\n",
    "plt.pie(sales_category.values, labels=sales_category.index, startangle=90, counterclock=False,\n",
    "        autopct=lambda p:f'{p:.1f}% \\n £{p*np.sum(sales_category.values)/100 :,.0f}', \n",
    "        wedgeprops={'linewidth': 1, 'edgecolor':'black', 'alpha':0.75});\n",
    "plt.axis('square');\n",
    "plt.title('Total Sales by Category',  fontdict={'fontsize':16});\n",
    "\n",
    "# right total profits pie chart\n",
    "plt.subplot(1,2,2); # 1 row, 2 columns, the 2nd plot\n",
    "plt.pie(profit_category.values, labels=profit_category.index, startangle=90, counterclock=False,\n",
    "        autopct=lambda p:f'{p:.1f}% \\n ${p*np.sum(profit_category.values)/100 :,.0f}',\n",
    "        wedgeprops={'linewidth': 1, 'edgecolor':'black', 'alpha':0.75});\n",
    "plt.axis('square');\n",
    "plt.title('Total Profit by Category', fontdict={'fontsize':16});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > _Total Sales on Categories_\n",
    "\n",
    "1. Technology at 36.4% \n",
    "2. Furniture at 32.3%\n",
    "3. Office Supplies at 31.3%\n",
    "\n",
    "Sales depict a near-perfect symmmetery on categories, with **Technology** winning with a slight edge.\n",
    "\n",
    " > _Total Profits on Categories_\n",
    "\n",
    "1. Technology at 50.1%\n",
    "2. Office Supplies at 42.7%\n",
    "3. Furniture at 7.2%\n",
    "\n",
    "Profits are largely taken by **Technology** category with *Office Supplies* being the lowest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping Sales/Profits based on Sub-Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sub_categories = global_super_store_data['Sub-Category'].unique()\n",
    "unique_sub_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total sales by sub-category from the highest sale.\n",
    "sales_sub_category = global_super_store_data.groupby(['Sub-Category'], as_index=False)['Sales'].sum().sort_values(by='Sales', ascending=False)\n",
    "sales_sub_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total profit by sub-category from the highest profit.\n",
    "profit_sub_category = global_super_store_data.groupby(['Sub-Category'], as_index=False)['Profit'].sum().sort_values(by='Sub-Category')\n",
    "profit_sub_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping the data on category and it's respective sub-categories. Calculating the profit margin.\n",
    "sales_per_cat_subcat = global_super_store_data.groupby(['Category', 'Sub-Category'], as_index=False)[['Sales', 'Profit']].sum()\n",
    "sales_per_cat_subcat['Profit Margin %'] = (sales_per_cat_subcat['Profit'] / sales_per_cat_subcat['Sales']) * 100\n",
    "\n",
    "#Sorting the dataframe based on profit margin\n",
    "sales_per_cat_subcat = sales_per_cat_subcat.sort_values(by=['Profit Margin %'], ascending=False)\n",
    "sales_per_cat_subcat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping Sales/Profits based on States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total sales by states from the highest sale.\n",
    "sales_by_states = global_super_store_data.groupby(['State'], as_index=False)['Sales'].sum().sort_values(by='Sales', ascending=False)\n",
    "sales_by_states['Sales %'] = (sales_by_states['Sales'] / global_super_store_data['Sales'].sum()) * 100\n",
    "print(sales_by_states.describe())\n",
    "sales_by_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **State which made the highest sales:** California\n",
    "> \n",
    "> **State which made the lowest sales:** North Dakota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping Sales/Profits based on Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total sales by segments from the highest sale.\n",
    "sales_by_segment = global_super_store_data.groupby(['Segment'], as_index=False)['Sales'].sum().sort_values(by='Sales', ascending=False)\n",
    "sales_by_segment['Sales %'] = (sales_by_segment['Sales'] / global_super_store_data['Sales'].sum()) * 100\n",
    "sales_by_segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consumer** Segment has 50% of Sales share, followed by **Corporate** and **Home Office**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping Sales/Profits based on Shipping Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total sales by segments from the highest sale.\n",
    "sales_by_ship_mode = global_super_store_data.groupby(['Ship Mode'], as_index=False)['Sales'].sum().sort_values(by='Sales', ascending=False)\n",
    "sales_by_ship_mode['Sales %'] = (sales_by_ship_mode['Sales'] / global_super_store_data['Sales'].sum()) * 100\n",
    "sales_by_ship_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard** shipping method is preferred as the sales percentage is nearly 60%, followed by **Second Class** and **First Class**. \n",
    "**Same Day** is not an economical option, so only preferred by 5% of the orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_super_store_data['Order Date'] = global_super_store_data['Order Date'].str.replace('$April', '')\n",
    "global_super_store_data['Order Date'] = pd.to_datetime(global_super_store_data['Order Date'], format='%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_super_store_data['Ship Date'] = pd.to_datetime(global_super_store_data['Ship Date'], format='%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_super_store_data['Shipment Days'] = (global_super_store_data['Ship Date'] - global_super_store_data['Order Date']).dt.days\n",
    "\n",
    "sales_by_ship_mode_days = global_super_store_data.groupby(['Ship Mode'], as_index=False)['Shipment Days'].sum().sort_values(by='Shipment Days', ascending=False)\n",
    "sales_by_ship_mode_days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to swap months if they are different\n",
    "def swap_month_if_different(row):\n",
    "    reference_dt = pd.Timestamp(row['Ship Date'])  # Reference datetime to compare months\n",
    "    \n",
    "    if row['Order Date'].month != reference_dt.month:\n",
    "        # Swap the month of the datetime column\n",
    "        row['Order Date'] = row['Order Date'].replace(month=reference_dt.month)\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Order ID         5231\n",
       "Order Date       5229\n",
       "Ship Date        5229\n",
       "Ship Mode        5229\n",
       "Customer ID      5232\n",
       "Customer Name    5229\n",
       "Segment          5230\n",
       "Country          5232\n",
       "City             5232\n",
       "State            5232\n",
       "Postal Code      5230\n",
       "Region           5231\n",
       "Product ID       5230\n",
       "Category         5232\n",
       "Sub-Category     5232\n",
       "Product Name     5231\n",
       "Sales            5231\n",
       "Quantity         5229\n",
       "Discount         5231\n",
       "Profit           5232\n",
       "Shipment Days    5226\n",
       "dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acceptable_shipment_days = np.arange(1, 29)\n",
    "\n",
    "# Get the all the records out of that filter to fix\n",
    "order_date_to_be_fixed_df = global_super_store_data[~global_super_store_data['Shipment Days'].isin(acceptable_shipment_days)]\n",
    "\n",
    "# TODO TBC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate analysis and visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Profit Margin by Sub-categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping the data on category and it's respective sub-categories. Calculating the profit margin.\n",
    "sales_per_cat_subcat = global_super_store_data.groupby(['Category', 'Sub-Category'], as_index=False)[['Sales', 'Profit']].sum()\n",
    "sales_per_cat_subcat['Profit Margin'] = sales_per_cat_subcat['Profit'] / sales_per_cat_subcat['Sales']\n",
    "\n",
    "#Sorting the dataframe based on profit margin\n",
    "sales_per_cat_subcat = sales_per_cat_subcat.sort_values(by=['Category', 'Sub-Category', 'Profit Margin'], ascending=True)\n",
    "\n",
    "# plot a profit margins sub-category bar chart \n",
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "\n",
    "# Unique sub categories without NO_SUB_CATEGORY\n",
    "unique_sub_categories_without_custom_label = sales_per_cat_subcat[sales_per_cat_subcat['Sub-Category'] != 'NO_SUB_CATEGORY']['Sub-Category'].unique()\n",
    "\n",
    "#Plotting the profit margin per sub-category.\n",
    "sns.barplot(y=sales_per_cat_subcat['Sub-Category'], x=sales_per_cat_subcat['Profit Margin'], hue=sales_per_cat_subcat['Category'], \n",
    "                alpha=1, dodge=False, ax=ax, order=unique_sub_categories_without_custom_label)\n",
    "\n",
    "#Cleaning out bar junk\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines[['right','top']].set_visible(False)\n",
    "ax.set(ylabel=None, xlabel='Profit Margin (%)')\n",
    "\n",
    "def move_ylabel_tick(index: list):\n",
    "    \"\"\"\n",
    "    Moving the provided ylabel ticks\n",
    "    \"\"\"\n",
    "    for tick in index:\n",
    "        ax.get_yticklabels()[tick].set_x(0.02)\n",
    "        ax.get_yticklabels()[tick].set_horizontalalignment('right')\n",
    "\n",
    "#Moving the y-labels on sub-categories that are making a loss in order to prevent collision of the bar and the text.\n",
    "move_ylabel_tick([-1, -2, -3])\n",
    "\n",
    "#Annotating the profit margin amount for each bar.\n",
    "for p in ax.patches:\n",
    "    _, y = p.get_xy()\n",
    "    \n",
    "    ax.annotate(f'{p.get_width()*100 :.1f}%', (p.get_width() / 2, y + 0.45))\n",
    "    \n",
    "#Calculating Superstore's aggregate profit margin in order to compare it to each sub-category's profit margin\n",
    "mean_profit = sales_per_cat_subcat['Profit'].sum() / sales_per_cat_subcat['Sales'].sum()\n",
    "\n",
    "#Plotting a vertical line and annotating the Superstore's aggregate profit margin.\n",
    "ax.axvline(mean_profit, color='blue', label='Mean Profit, All Categories', alpha=0.75, ls='-.')\n",
    "\n",
    "#Setting the title and legend.\n",
    "ax.set_title('Profit Margin by Sub-Category', fontdict={'fontsize':16})\n",
    "ax.legend(loc=(1, 0.9))\n",
    "\n",
    "#Formatting the x-axis as %\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geo Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder_path = os.getcwd()\n",
    "shp_path = '/data/usa-states-census-2014.shp'\n",
    "\n",
    "states = gpd.read_file(f'{base_folder_path}{shp_path}')\n",
    "states = states.to_crs(\"EPSG:3395\")\n",
    "\n",
    "# Group data based on States\n",
    "sales_per_states = global_super_store_data.groupby(['State'], as_index=False)[['Sales']].sum()\n",
    "\n",
    "# Create a new column to see the Sales %\n",
    "sales_per_states['Sales %'] = (sales_per_states['Sales'] / global_super_store_data['Sales'].sum()) * 100\n",
    "\n",
    "# Merge sales data with the US map based on state codes or names\n",
    "merged_data = states.merge(sales_per_states, how='left', left_on='NAME', right_on='State')\n",
    "\n",
    "# Plot the map\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "states.plot(ax=ax, color='lightgrey', edgecolor='black')\n",
    "merged_data.plot(column='Sales %', cmap='Spectral', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True, aspect='equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('Sales by US State', loc='center', fontsize=14, y=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time series analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select Numerical Features\n",
    "numerical_features = ['Sales', 'Quantity', 'Discount', 'Profit']\n",
    "global_super_store_numerial_data = global_super_store_data[numerical_features]\n",
    "\n",
    "correlation_matrix = global_super_store_numerial_data.corr()\n",
    "correlation_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot correlation matrix as heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interquartile Range (IQR) Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['Sales', 'Quantity', 'Discount', 'Profit']\n",
    "\n",
    "for numerical_feature in numerical_features:\n",
    "    q1 = global_super_store_data[numerical_feature].quantile(0.25)\n",
    "    q3 = global_super_store_data[numerical_feature].quantile(0.75)\n",
    "    \n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    lower_limit = q1 - 1.5 * iqr\n",
    "    upper_limit = q3 + 1.5* iqr\n",
    "\n",
    "    outliers_df = global_super_store_data[(global_super_store_data[numerical_feature] < lower_limit)|(global_super_store_data[numerical_feature] > upper_limit)]\n",
    "    sns.boxplot(outliers_df)\n",
    "    print(f\"Outlier numerical feature: {numerical_feature}, Outlier Count: {outliers_df[numerical_feature].count()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
