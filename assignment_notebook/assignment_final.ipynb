{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOBAL SUPERSTORE DATASET SALES ANALYSIS\n",
    "By Raju Vaneshwar Nareshwar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load first 10 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install packages\n",
    "import sys\n",
    "!{sys.executable} -m pip install geopandas\n",
    "!{sys.executable} -m pip install adjustText\n",
    "!{sys.executable} -m pip install gender-guesser\n",
    "\n",
    "# import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import socket\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "import gender_guesser.detector as gender\n",
    "\n",
    "from matplotlib import ticker as mtick\n",
    "from tabulate import tabulate\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Letting pandas to show max columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Set the style for seaborn plots\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "detector = gender.Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV file and assigning into a dataframe ss_data\n",
    "global_super_store_df_org = pd.read_csv('sample-superstore-2023-T3.csv')\n",
    "\n",
    "# Copy the dataframe before processing\n",
    "global_super_store_df = global_super_store_df_org.copy();\n",
    "\n",
    "# Set the head to 10 to retrieve the first 10 records\n",
    "first_10_rows = global_super_store_df.head(n=10)\n",
    "first_10_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information of descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using info() and describe() function to get the descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metadata information about the dataset\n",
    "# Grouping data types by category\n",
    "numerical_columns = global_super_store_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_columns = global_super_store_df.select_dtypes(include=['object']).columns\n",
    "datetime_columns = global_super_store_df.select_dtypes(include=['datetime64']).columns\n",
    "\n",
    "# Define a table width and print the header row with a dotted line\n",
    "table_width = 200\n",
    "print(\"-\" * table_width)\n",
    "\n",
    "# Print each row of the table with content and a dash line\n",
    "content_list = [f\"We are working with a {global_super_store_df.shape} sized dataset.\", \n",
    "                    f\"Numercial columns: {', '.join(numerical_columns)}\",\n",
    "                    f\"Categorical columns: {', '.join(categorical_columns)}\",\n",
    "                    f\"Date Time columns: {', '.join(datetime_columns)}\"]\n",
    "\n",
    "for content_row in content_list:\n",
    "    print(\"| {:<196} |\".format(content_row))\n",
    "    print(\"-\" * table_width)\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(global_super_store_df.describe(), tablefmt='pretty', stralign ='left', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metatype information about the dataset\n",
    "print(global_super_store_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of missing values in each column\n",
    "missing_values_per_column = global_super_store_df.isna().sum()\n",
    "\n",
    "# Convert the Series to a DataFrame for tabulation\n",
    "missing_values_df = missing_values_per_column.to_frame().reset_index()\n",
    "missing_values_df.columns = ['Column', 'Missing Values']\n",
    "\n",
    "# Print the tabulated missing values per column\n",
    "print(\"Number of missing values per column:\\n\")\n",
    "print(tabulate(missing_values_df, headers='keys', tablefmt='pretty', stralign ='left', showindex=False))\n",
    "\n",
    "# Count the total number of missing values across all columns\n",
    "total_missing_values = missing_values_per_column.sum()\n",
    "\n",
    "# Print the total number of missing values\n",
    "print(\"\\nTotal number of missing values:\", total_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary key of these records are a system-generated, and denoted as column: *RowID*\n",
    "\n",
    "The datatypes of the dataset are following:\n",
    "* int64(1)\n",
    "* float64(2)\n",
    "* object(18)\n",
    "\n",
    "A few records of *Quantity* and *Profit* columns has the datatype of object, but it must be float64, thus needs to be cleansed or transformed.  \n",
    "\n",
    "*Ship Date* and *Order Date* columns are represented as strings, those needs to be converted as datetime.\n",
    "\n",
    "Once cleansed, the descriptive statistics can be applied to the numerial columns, and they are Sales, Quantity, Discount and Profit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms based on categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(data, xlabel, ylabel, title): \n",
    "    \n",
    "    \"\"\"    \n",
    "    plot_histogram generates a histogram plot for the given dataset.\n",
    "    Parameters:\n",
    "      data: Dataset for the histogram plot (pandas Series or any iterable).\n",
    "      xlabel: Label for the x-axis.\n",
    "      ylabel: Label for the y-axis.\n",
    "      title: Title of the plot.\n",
    "    Returns: None\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(12, 10)) \n",
    "\n",
    "    bars = plt.bar(data.index, data)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, height, height, ha='center', va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for 'Ship Mode'\n",
    "orders_per_segment = global_super_store_df['Ship Mode'].value_counts()\n",
    "plot_histogram(orders_per_segment, 'Ship Mode', 'Number of Orders', 'Number of Orders per Ship Mode')\n",
    "\n",
    "# Histogram for 'Segment'\n",
    "orders_per_segment = global_super_store_df['Segment'].value_counts()\n",
    "plot_histogram(orders_per_segment, 'Segment', 'Number of Orders', 'Number of Orders per Segment')\n",
    "\n",
    "# Histogram for 'Category'\n",
    "orders_per_category = global_super_store_df['Category'].value_counts()\n",
    "plot_histogram(orders_per_category, 'Category', 'Number of Orders', 'Number of Orders per Category')\n",
    "\n",
    "# Histogram for 'Sub-Category'\n",
    "orders_per_sub_category = global_super_store_df['Sub-Category'].value_counts()\n",
    "plot_histogram(orders_per_sub_category, 'Sub-Category', 'Number of Orders', 'Number of Orders per Sub-Category')\n",
    "\n",
    "# Histogram for 'Region'\n",
    "orders_per_sub_category = global_super_store_df['Region'].value_counts()\n",
    "plot_histogram(orders_per_sub_category, 'Region', 'Number of Orders', 'Number of Orders per Region')\n",
    "\n",
    "# Histogram for 'State'\n",
    "orders_per_sub_category = global_super_store_df['State'].value_counts()\n",
    "plot_histogram(orders_per_sub_category, 'State', 'Number of Orders', 'Number of Orders per State')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **plot_histogram()** function can be run after the data cleansing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing values in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Before performing any statistical analysis, the numerical column data has to be cleansed to be meaningful.\n",
    "* Records with special characters on **Quantity** and needs to be cleansed. \n",
    "* Records with special characters on **Profit** and needs to be cleansed. \n",
    "* Applying the **text2float()** function to fix *Quantity* column. \n",
    "* City and States are missing on a few records, **get_city_from_postal_code()** and **get_state_from_postal_code()** functions will fix them via API.\n",
    "* **Category** and **Sub-Category** needs cleansing and reorganizing.\n",
    "* Order Date and Ship Date is jumbled between dd-mm-yyyy, and mm-dd-yyyy formats.\n",
    "* New columns to perform EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2float(textnum, numwords={}):\n",
    "    \"\"\"\n",
    "    text2float converts the textual representation of numbers to float.\n",
    "    Parameters:\n",
    "        textnum: Textual representation of a number.\n",
    "        numwords: Dictionary mapping words to their numeric values.\n",
    "    Returns:\n",
    "        Float: The numerical value represented by the input text.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Attempt to convert to float\n",
    "        return float(textnum)\n",
    "    except ValueError:\n",
    "        # If conversion to float fails, continue with text to number conversion\n",
    "        textnum = textnum.lower()\n",
    "        \n",
    "        if not numwords:\n",
    "            units = [\n",
    "                \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
    "                \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "                \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\",\n",
    "            ]\n",
    "\n",
    "            tens = [\"\", \"\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"]\n",
    "\n",
    "            scales = [\"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n",
    "\n",
    "            numwords[\"and\"] = (1, 0)\n",
    "            for idx, word in enumerate(units):\n",
    "                numwords[word] = (1, idx)\n",
    "            for idx, word in enumerate(tens):\n",
    "                numwords[word] = (1, idx * 10)\n",
    "            for idx, word in enumerate(scales):\n",
    "                numwords[word] = (10 ** (idx * 3 or 2), 0)\n",
    "\n",
    "        current = result = 0\n",
    "        for word in textnum.split():\n",
    "            if word not in numwords:\n",
    "                raise Exception(\"Illegal word: \" + word)\n",
    "\n",
    "            scale, increment = numwords[word]\n",
    "            current = current * scale + increment\n",
    "            if scale > 100:\n",
    "                result += current\n",
    "                current = 0\n",
    "\n",
    "        return result + current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_internet_connected():\n",
    "    \"\"\"\n",
    "    Check if the machine is connected to the internet by attempting to connect to Google's DNS server.\n",
    "    Returns:\n",
    "        bool: True if the machine is connected to the internet, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to connect to Google's DNS server\n",
    "        socket.create_connection((\"8.8.8.8\", 53), timeout=3)\n",
    "        return True\n",
    "    except OSError:\n",
    "        pass\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_from_postal_code(postal_code):\n",
    "    \"\"\"\n",
    "    get_city_from_postal_code retrieves the city name from postal code using zippopotam API.\n",
    "    Parameters:\n",
    "        postal_code: Postal code for the city.\n",
    "    Returns:\n",
    "        str or None: City name corresponding to the given postal code, or None if postal_code is empty or invalid.\n",
    "    \"\"\"\n",
    "    if not is_internet_connected():\n",
    "        return None\n",
    "\n",
    "    if postal_code == '':\n",
    "        return None\n",
    "\n",
    "    url = f\"http://api.zippopotam.us/us/{postal_code}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        city = data['places'][0]['place name']\n",
    "        return city\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_from_postal_code(postal_code):\n",
    "    \"\"\"\n",
    "    get_state_from_postal_code retrieves the state name from postal code using zippopotam API.\n",
    "    Parameters:\n",
    "        postal_code: Postal code for the state.\n",
    "    Returns:\n",
    "        str or None: State name corresponding to the given postal code, or None if postal_code is empty or invalid.\n",
    "    \"\"\"\n",
    "\n",
    "    if not is_internet_connected():\n",
    "        return None\n",
    "\n",
    "    if postal_code == '':\n",
    "        return None\n",
    "\n",
    "    url = f\"http://api.zippopotam.us/us/{postal_code}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        state = data['places'][0]['state']\n",
    "        return state\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_date_in_yyyymmdd(date_str):\n",
    "    \"\"\"\n",
    "    Formats a date string to the format 'yyyy-mm-dd'.\n",
    "\n",
    "    Parameters:\n",
    "    - date_str (str or pd.NA): A string representing a date.\n",
    "\n",
    "    Returns:\n",
    "    - str or pd.NA: The standardized date string in 'yyyy-mm-dd' format, or pd.NA if input is blank or NaN.\n",
    "    \"\"\"\n",
    "    if pd.notna(date_str) and date_str.strip():  # Check if date is not blank or NaN\n",
    "        try:\n",
    "            # Attempt to parse as yyyy-mm-dd with dayfirst=False\n",
    "            return pd.to_datetime(date_str, dayfirst=False, errors='coerce').strftime('%Y-%m-%d')\n",
    "        except ValueError:\n",
    "            # If parsing fails, attempt to parse as dd-mm-yyyy\n",
    "            return pd.to_datetime(date_str, format='%d-%m-%Y', errors='coerce').strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        return date_str  # Return the original blank or NaN value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_gender(full_name):\n",
    "    \"\"\"\n",
    "    Guesses the gender based on the provided full name.\n",
    "\n",
    "    Parameters:\n",
    "    - full_name (str or pd.NA): The full name of the person.\n",
    "\n",
    "    Returns:\n",
    "    - str: The guessed gender ('Male', 'Female', or 'Unknown').\n",
    "    \"\"\"\n",
    "    if pd.notna(full_name):  # Check if the value is not NaN\n",
    "        parts = full_name.split()\n",
    "        if len(parts) > 1:  # Check if the name contains a space\n",
    "            first_name, last_name = parts[:2]  # Split into first name and last name\n",
    "            first_gender = detector.get_gender(first_name)\n",
    "            last_gender = detector.get_gender(last_name)\n",
    "\n",
    "            # If either part indicates male, return Male\n",
    "            if first_gender == 'male' or last_gender == 'male':\n",
    "                return 'Male'\n",
    "            # If either part indicates female, return Female\n",
    "            elif first_gender == 'female' or last_gender == 'female':\n",
    "                return 'Female'\n",
    "            # If neither part indicates a gender, return Unknown\n",
    "            else:\n",
    "                return 'Unknown'\n",
    "        else:\n",
    "            # Only one name provided, guess gender based on it\n",
    "            single_name_gender = detector.get_gender(full_name.split()[0])\n",
    "            if single_name_gender == 'male':\n",
    "                return 'Male'\n",
    "            elif single_name_gender == 'female':\n",
    "                return 'Female'\n",
    "            else:\n",
    "                return 'Unknown'\n",
    "    else:\n",
    "        return 'Unknown'  # Return 'Unknown' for NaN values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row ID is not needed for the analysis, hence dropping the column\n",
    "if 'Row ID' in global_super_store_df.columns:\n",
    "    global_super_store_df.drop('Row ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling empty values with default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming zero values for NaN on Profits\n",
    "global_super_store_df['Profit'] = global_super_store_df['Profit'].fillna(0)\n",
    "\n",
    "# Filling values on empty Category and Sub-Category records\n",
    "global_super_store_df['Category'] = global_super_store_df['Category'].fillna('NO_CATEGORY')\n",
    "global_super_store_df['Sub-Category'] = global_super_store_df['Sub-Category'].fillna('NO_SUB_CATEGORY')\n",
    "\n",
    "# Filling values on empty Customer Name records\n",
    "global_super_store_df['Customer Name'].fillna('NONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing faulty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing \"?\" from Quantity column\n",
    "global_super_store_df['Quantity'] = global_super_store_df['Quantity'].str.replace('?', '')\n",
    "\n",
    "# Removing \"\"\" from Profit column\n",
    "global_super_store_df['Profit'] = global_super_store_df['Profit'].str.replace('\"', '')\n",
    "\n",
    "# Removing \"\"\" from Postal Code column\n",
    "global_super_store_df['Postal Code'] = global_super_store_df['Postal Code'].str.replace('\"', '')\n",
    "\n",
    "# Make all records as Country = United States\n",
    "global_super_store_df['Country'] = 'United States'\n",
    "\n",
    "# Correcting spelling mistakes on Category column\n",
    "global_super_store_df['Category'] = global_super_store_df['Category'].replace('Frnture', 'Furniture')\n",
    "\n",
    "# Datafix on Category based on subcategories\n",
    "# Apply the condition element-wise\n",
    "condition = (global_super_store_df['Category'] == 'NO_CATEGORY') & \\\n",
    "            (global_super_store_df['Sub-Category'].isin(['Binders', 'Storage']))\n",
    "\n",
    "# Update 'Category' where the condition is True\n",
    "global_super_store_df.loc[condition, 'Category'] = 'Office Supplies'\n",
    "\n",
    "# Update empty and wrong records on Segment Column\n",
    "global_super_store_df['Segment'] = global_super_store_df['Segment'].replace('%', '')\n",
    "global_super_store_df['Segment'] = global_super_store_df['Segment'].replace('', 'NO_SEGMENT')\n",
    "\n",
    "# Cleanse the Regions\n",
    "central_regions_to_replace = ['Centrl', 'Cntral']\n",
    "east_regions_to_replace = ['Est']\n",
    "south_regions_to_replace = ['Southh']\n",
    "\n",
    "global_super_store_df['Region'] = global_super_store_df['Region'].replace(central_regions_to_replace, 'Central')\n",
    "global_super_store_df['Region'] = global_super_store_df['Region'].replace(east_regions_to_replace, 'East')\n",
    "global_super_store_df['Region'] = global_super_store_df['Region'].replace(south_regions_to_replace, 'South')\n",
    "\n",
    "# Cleanse the Order Date column\n",
    "global_super_store_df['Order Date'] = global_super_store_df['Order Date'].str.replace('$April', '')\n",
    "\n",
    "# Cleanse the Customer Name column\n",
    "global_super_store_df['Customer Name'] = global_super_store_df['Customer Name'].str.replace('10', 'NONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying helper functions to fill the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the empty state rows\n",
    "state_filtered_na = global_super_store_df.loc[pd.isna(global_super_store_df['State'])]\n",
    "state_filtered_na\n",
    "\n",
    "# Apply the function to fill the missing value via API\n",
    "global_super_store_df.loc[pd.isna(global_super_store_df['State']), 'State'] = state_filtered_na['Postal Code'].apply(get_state_from_postal_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the empty city rows\n",
    "city_filtered_na = global_super_store_df.loc[pd.isna(global_super_store_df['City'])]\n",
    "city_filtered_na\n",
    "\n",
    "# Apply the function to fill the missing value via API\n",
    "global_super_store_df.loc[pd.isna(global_super_store_df['City']), 'City'] = city_filtered_na['Postal Code'].apply(get_city_from_postal_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying text2float function\n",
    "global_super_store_df['Quantity'] = global_super_store_df['Quantity'].apply(text2float)\n",
    "global_super_store_df['Profit'] = global_super_store_df['Profit'].apply(text2float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Order Date / Ship Date fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the all the records out of that filter to fix\n",
    "date_fix_condition = (~global_super_store_df['Ship Date'].isna()) & \\\n",
    "                            (~global_super_store_df['Order Date'].isna())\n",
    "\n",
    "# Standardize Order Date column\n",
    "global_super_store_df.loc[date_fix_condition, 'Order Date'] = global_super_store_df.loc[date_fix_condition, 'Order Date'].apply(format_date_in_yyyymmdd)\n",
    "\n",
    "# Standardize Ship Date column\n",
    "global_super_store_df.loc[date_fix_condition, 'Ship Date'] = global_super_store_df.loc[date_fix_condition, 'Ship Date'].apply(format_date_in_yyyymmdd)\n",
    "\n",
    "# Swap Order Date and Ship Date if necessary\n",
    "for index, row in global_super_store_df.iterrows():\n",
    "    order_date = pd.to_datetime(row['Order Date'], errors='coerce')  # Coerce errors to NaT for comparison\n",
    "    ship_date = pd.to_datetime(row['Ship Date'], errors='coerce')  # Coerce errors to NaT for comparison\n",
    "    \n",
    "    if not pd.isna(order_date) and not pd.isna(ship_date) and order_date > ship_date:\n",
    "        global_super_store_df.at[index, 'Order Date'], global_super_store_df.at[index, 'Ship Date'] = row['Ship Date'], row['Order Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling 01/01/1970 date as a filler date\n",
    "global_super_store_df['Order Date'] = global_super_store_df['Order Date'].fillna('01/01/1970')\n",
    "global_super_store_df['Ship Date'] = global_super_store_df['Ship Date'].fillna('01/01/1970')\n",
    "\n",
    "\n",
    "# Convert columns to datetime64 dtype\n",
    "global_super_store_df['Order Date'] = pd.to_datetime(global_super_store_df['Order Date'], format='mixed')\n",
    "global_super_store_df['Ship Date'] = pd.to_datetime(global_super_store_df['Ship Date'], format='mixed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New columns to Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleansing the data, the following columns have been added to perform EDA and visualizations.\n",
    "\n",
    "| Column Name   | Data Type | Description                                                                   |\n",
    "| ------------  | --------- |-------------------------------------------------------------------------------|\n",
    "| Shipment Days | float64   | Calculate the days between Order Date and Shipped Date                        |                                         \n",
    "| Order Year    | int32     | Extracted year from Order Date to plot the sales/profit trends                |\n",
    "| Ship Year     | int32     | Extracted year from Ship Date to plot the sales/profit trends                 |                       \n",
    "| Gender        | object    | Experimenting to guess the gender of the customer to perform gender analysis  |                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New column Shipment Days\n",
    "global_super_store_df['Shipment Days'] = (global_super_store_df['Ship Date'] - global_super_store_df['Order Date']).dt.days\n",
    "\n",
    "# New columns: Order Year and Ship Year. extract year from 'Order Date' and 'Ship Date' columns\n",
    "global_super_store_df['Order Year'] = pd.DatetimeIndex(global_super_store_df['Order Date']).year\n",
    "global_super_store_df['Ship Year'] = pd.DatetimeIndex(global_super_store_df['Ship Date']).year\n",
    "\n",
    "# Fill NaN values with 0 (assuming missing years should be represented as 0)\n",
    "global_super_store_df[['Order Year', 'Ship Year']] = global_super_store_df[['Order Year', 'Ship Year']].fillna(0)\n",
    "\n",
    "# Convert year columns to integer dtype\n",
    "global_super_store_df[['Order Year', 'Ship Year']] = global_super_store_df[['Order Year', 'Ship Year']].astype(int)\n",
    "\n",
    "# Guessing the gender of the Customer\n",
    "global_super_store_df['Gender'] = global_super_store_df['Customer Name'].apply(guess_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting histograms after cleansing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for 'Ship Mode'\n",
    "orders_per_segment = global_super_store_df['Ship Mode'].value_counts()\n",
    "plot_histogram(orders_per_segment, 'Ship Mode', 'Number of Orders', 'Number of Orders per Ship Mode')\n",
    "\n",
    "# Histogram for 'Segment'\n",
    "orders_per_segment = global_super_store_df['Segment'].value_counts()\n",
    "plot_histogram(orders_per_segment, 'Segment', 'Number of Orders', 'Number of Orders per Segment')\n",
    "\n",
    "# Histogram for 'Category'\n",
    "orders_per_category = global_super_store_df['Category'].value_counts()\n",
    "plot_histogram(orders_per_category, 'Category', 'Number of Orders', 'Number of Orders per Category')\n",
    "\n",
    "# Histogram for 'Sub-Category'\n",
    "orders_per_sub_category = global_super_store_df['Sub-Category'].value_counts()\n",
    "plot_histogram(orders_per_sub_category, 'Sub-Category', 'Number of Orders', 'Number of Orders per Sub-Category')\n",
    "\n",
    "# Histogram for 'Region'\n",
    "orders_per_sub_category = global_super_store_df['Region'].value_counts()\n",
    "plot_histogram(orders_per_sub_category, 'Region', 'Number of Orders', 'Number of Orders per Region')\n",
    "\n",
    "# Histogram for 'State'\n",
    "orders_per_sub_category = global_super_store_df['State'].value_counts()\n",
    "plot_histogram(orders_per_sub_category, 'State', 'Number of Orders', 'Number of Orders per State')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_super_store_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with missing data\n",
    "print(f\"Sum of null records:\\n{global_super_store_df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping Sales/Profits based on Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_unique_values_on_tabulate(df, column_name):\n",
    "    \"\"\"\n",
    "    print_unique_values_on_tabulate, prints unique values from a specified column of a DataFrame in a table format.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame from which unique values will be extracted.\n",
    "    - column_name (str): The name of the column from which unique values will be extracted.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Get unique values from the specified column\n",
    "    unique_values = df[column_name].unique()\n",
    "    \n",
    "    # Flatten unique values array\n",
    "    flat_unique_values_array = unique_values.flatten()\n",
    "    \n",
    "    # Convert flattened array to list of lists\n",
    "    data = [[item] for item in flat_unique_values_array]\n",
    "    \n",
    "    # Set Header\n",
    "    header = [f'Unique {column_name.capitalize()}']\n",
    "    \n",
    "    # Print the table using tabulate\n",
    "    print(tabulate(data, headers=header, tablefmt='pretty', stralign ='left', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_unique_values_on_tabulate(global_super_store_df, 'Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sales_profit_by_feature(df, feature_column, category_column):\n",
    "    \"\"\"\n",
    "    print_sales_profit_by_feature, prints total sales/[rpfit] by category in a tabular format.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing the sales data.\n",
    "    - sales_column (str): The name of the column containing sales data.\n",
    "    - sales_category_column (str): The name of the column to group by.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Group total sales by category\n",
    "    sales_category = df.groupby(category_column)[feature_column].sum().round(4).reset_index().sort_values(by=feature_column, ascending=False)\n",
    "    \n",
    "    # Convert the grouped DataFrame to a list of lists\n",
    "    sales_category_list = sales_category.values.tolist()\n",
    "\n",
    "    # Print the table using tabulate\n",
    "    print(tabulate(sales_category_list, headers=[category_column, 'Total Sales'], tablefmt='pretty', stralign ='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total sales by category from the highest sale.\n",
    "print_sales_profit_by_feature(global_super_store_df.round(2), 'Sales', 'Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total sales by category from the highest sale.\n",
    "print_sales_profit_by_feature(global_super_store_df.round(2), 'Profit', 'Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total sales by category, only considering positive sales\n",
    "positive_sales_category = global_super_store_df.groupby('Category').filter(lambda x: x['Sales'].sum() > 0).groupby('Category')['Sales'].sum()\n",
    "\n",
    "# group total profit by category, only considering positive profits\n",
    "positive_profit_category = global_super_store_df.groupby('Category').filter(lambda x: x['Profit'].sum() > 0).groupby('Category')['Profit'].sum()\n",
    "\n",
    "# figure size\n",
    "plt.figure(figsize=(15,10));\n",
    "\n",
    "# left total sales pie chart\n",
    "plt.subplot(1,2,1); # 1 row, 2 columns, the 1st plot.\n",
    "plt.pie(positive_sales_category.values, labels=positive_sales_category.index, startangle=90, counterclock=False,\n",
    "        autopct=lambda p:f'{p:,.2f}% \\n ${p * np.sum(positive_sales_category.values) / 100 :,.2f}', \n",
    "        wedgeprops={'linewidth': 1, 'edgecolor':'black', 'alpha':0.75});\n",
    "plt.axis('square');\n",
    "plt.title('Total Sales by Category',  fontdict={'fontsize':16});\n",
    "\n",
    "# right total profits pie chart\n",
    "plt.subplot(1,2,2); # 1 row, 2 columns, the 2nd plot\n",
    "plt.pie(positive_profit_category.values, labels=positive_profit_category.index, startangle=90, counterclock=False,\n",
    "        autopct=lambda p:f'{p:,.2f}% \\n ${p * np.sum(positive_profit_category.values) / 100 :,.2f}',\n",
    "        wedgeprops={'linewidth': 1, 'edgecolor':'black', 'alpha':0.75});\n",
    "plt.axis('square');\n",
    "plt.title('Total Profit by Category', fontdict={'fontsize':16});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > _Total Sales on Categories_\n",
    "\n",
    "1. Technology at 36.4% \n",
    "2. Furniture at 32.3%\n",
    "3. Office Supplies at 31.3%\n",
    "\n",
    "Sales depict a near-perfect symmmetery on categories, with **Technology** winning with a slight edge.\n",
    "\n",
    " > _Total Profits on Categories_\n",
    "\n",
    "1. Technology at 50.1%\n",
    "2. Office Supplies at 42.7%\n",
    "3. Furniture at 7.2%\n",
    "\n",
    "Profits are largely taken by **Technology** category with *Office Supplies* being the lowest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping Sales/Profits based on Sub-Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_unique_values_on_tabulate(global_super_store_df, 'Sub-Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total sales by sub-category from the highest sale.\n",
    "print_sales_profit_by_feature(global_super_store_df.round(2), 'Sales', 'Sub-Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total profit by sub-category from the highest profit.\n",
    "print_sales_profit_by_feature(global_super_store_df.round(2), 'Profit', 'Sub-Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping the data on category and it's respective sub-categories. Calculating the profit margin.\n",
    "sales_per_cat_subcat = global_super_store_df.groupby(['Category', 'Sub-Category'], as_index=False)[['Sales', 'Profit']].sum()\n",
    "sales_per_cat_subcat['Profit Margin %'] = (sales_per_cat_subcat['Profit'] / sales_per_cat_subcat['Sales']) * 100\n",
    "\n",
    "#Sorting the dataframe based on profit margin\n",
    "sales_per_cat_subcat = sales_per_cat_subcat.sort_values(by=['Profit Margin %'], ascending=False)\n",
    "print(tabulate(sales_per_cat_subcat.round(2), headers='keys', tablefmt='pretty', stralign ='left', showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping Sales/Profits based on States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total sales by states from the highest sale.\n",
    "sales_by_states = global_super_store_df.groupby(['State'], as_index=False)['Sales'].sum().sort_values(by='Sales', ascending=False)\n",
    "sales_by_states['Sales %'] = (sales_by_states['Sales'] / global_super_store_df['Sales'].sum()) * 100\n",
    "\n",
    "# Convert numerical values to strings with commas for thousands separators and round to 2 decimal places\n",
    "for numerical_column in sales_by_states.select_dtypes(include='number').columns:\n",
    "    sales_by_states[numerical_column] = sales_by_states[numerical_column].apply(lambda x: '{:,.2f}'.format(x))\n",
    "\n",
    "# List of total sales by states\n",
    "print(tabulate(sales_by_states.round(2), headers='keys', tablefmt='pretty', stralign ='left', showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **State which made the highest sales:** California\n",
    "> \n",
    "> **State which made the lowest sales:** North Dakota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping Sales/Profits based on Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total sales by segments from the highest sale.\n",
    "sales_by_segment = global_super_store_df.groupby(['Segment'], as_index=False)['Sales'].sum().sort_values(by='Sales', ascending=False)\n",
    "sales_by_segment['Sales %'] = (sales_by_segment['Sales'] / global_super_store_df['Sales'].sum()) * 100\n",
    "\n",
    "# Convert numerical values to strings with commas for thousands separators and round to 2 decimal places\n",
    "for numerical_column in sales_by_segment.select_dtypes(include='number').columns:\n",
    "    sales_by_segment[numerical_column] = sales_by_segment[numerical_column].apply(lambda x: '{:,.2f}'.format(x))\n",
    "\n",
    "# List of total sales by states\n",
    "print(tabulate(sales_by_segment.round(2), headers='keys', tablefmt='pretty', stralign ='left', showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consumer** Segment has 50% of Sales share, followed by **Corporate** and **Home Office**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping Sales/Profits based on Shipping Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total sales by segments from the highest sale.\n",
    "sales_by_ship_mode = global_super_store_df.groupby(['Ship Mode'], as_index=False)['Sales'].sum().sort_values(by='Sales', ascending=False)\n",
    "sales_by_ship_mode['Sales %'] = (sales_by_ship_mode['Sales'] / global_super_store_df['Sales'].sum()) * 100\n",
    "\n",
    "# Convert numerical values to strings with commas for thousands separators and round to 2 decimal places\n",
    "for numerical_column in sales_by_segment.select_dtypes(include='number').columns:\n",
    "    sales_by_ship_mode[numerical_column] = sales_by_ship_mode[numerical_column].apply(lambda x: '{:,.2f}'.format(x))\n",
    "\n",
    "# List of total sales by states\n",
    "print(tabulate(sales_by_ship_mode.round(2), headers='keys', tablefmt='pretty', stralign ='left', showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard** shipping method is preferred as the sales percentage is nearly 60%, followed by **Second Class** and **First Class**. \n",
    "**Same Day** is not an economical option, so only preferred by 5% of the orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the filler date 01/01/1970 - the default NO_DATA date\n",
    "global_super_store_df_date_filtered = global_super_store_df[(global_super_store_df['Order Date'] != pd.to_datetime('01/01/1970')) & (global_super_store_df['Ship Date'] != pd.to_datetime('01/01/1970'))].groupby('Ship Mode')['Shipment Days'].mean()\n",
    "grouped_data = global_super_store_df_date_filtered.round(3).reset_index().values.tolist()\n",
    "\n",
    "print(tabulate(grouped_data, headers=['Ship Mode', 'Average Shipped in Days'], tablefmt='pretty', stralign ='left', showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Standard Class** shipping method takes ~5 days to ship from the order date.\n",
    "* **First Class** and **Second Class** are between 2-3 days. \n",
    "* **Same Day** is as promised, the orders are shipped with a day.\n",
    "\n",
    "In conclusion, **Second Class** ship mode is the best value for money, being economical than **First Class** and faster than **Standard Class**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate analysis and visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_density(df, column, xlim=None):\n",
    "    \"\"\"\n",
    "    Plot histogram with density plot for a specified column in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame.\n",
    "    column (str): Name of the column to plot.\n",
    "    xlim (tuple): Tuple containing the lower and upper limits of the x-axis.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Create subplots for each column\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "    # Plot histogram with density plot\n",
    "    sns.histplot(df[column], kde=True, ax=ax)\n",
    "    ax.set_title(f'Distribution of {column}')\n",
    "\n",
    "    # Set x-axis limits if specified\n",
    "    if xlim:\n",
    "        plt.xlim(xlim)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_density(global_super_store_df, 'Sales', xlim=(0, 2500))\n",
    "plot_histogram_density(global_super_store_df, 'Quantity')\n",
    "plot_histogram_density(global_super_store_df, 'Discount')\n",
    "plot_histogram_density(global_super_store_df, 'Profit', xlim=(-100, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDF/CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots for each column\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot PDF and CDF for Sales\n",
    "sns.kdeplot(global_super_store_df['Sales'], cumulative=False, ax=axes[0, 0], color='blue', label='PDF')\n",
    "sns.kdeplot(global_super_store_df['Sales'], cumulative=True, ax=axes[0, 0], color='red', label='CDF')\n",
    "axes[0, 0].set_title('PDF and CDF for Sales')\n",
    "\n",
    "# Plot PDF and CDF for Quantity\n",
    "sns.kdeplot(global_super_store_df['Quantity'], cumulative=False, ax=axes[0, 1], color='blue', label='PDF')\n",
    "sns.kdeplot(global_super_store_df['Quantity'], cumulative=True, ax=axes[0, 1], color='red', label='CDF')\n",
    "axes[0, 1].set_title('PDF and CDF for Quantity')\n",
    "\n",
    "# Plot PDF and CDF for Discount\n",
    "sns.kdeplot(global_super_store_df['Discount'], cumulative=False, ax=axes[1, 0], color='blue', label='PDF')\n",
    "sns.kdeplot(global_super_store_df['Discount'], cumulative=True, ax=axes[1, 0], color='red', label='CDF')\n",
    "axes[1, 0].set_title('PDF and CDF for Discount')\n",
    "\n",
    "# Plot PDF and CDF for Profit\n",
    "sns.kdeplot(global_super_store_df['Profit'], cumulative=False, ax=axes[1, 1], color='blue', label='PDF')\n",
    "sns.kdeplot(global_super_store_df['Profit'], cumulative=True, ax=axes[1, 1], color='red', label='CDF')\n",
    "axes[1, 1].set_title('PDF and CDF for Profit')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box_with_mean(df, column):\n",
    "    \"\"\"\n",
    "    Plot box plot with mean marked for a specified column in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame.\n",
    "    column (str): Name of the column to plot.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Create a box plot with mean marked\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.boxplot(x=df[column], showfliers=False, showmeans=True, meanprops={'marker': 'o', 'markerfacecolor': 'blue', 'markeredgecolor': 'blue'}, medianprops={'color': 'red'})\n",
    "    plt.title(f'Box plot of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Values')\n",
    "    plt.show()\n",
    "\n",
    "    # Create a box plot with mean marked\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.boxplot(x=df[column], showfliers=True, showmeans=True, meanprops={'marker': 'o', 'markerfacecolor': 'blue', 'markeredgecolor': 'blue'}, medianprops={'color': 'red'})\n",
    "    plt.title(f'Box plot of {column} with Outliers')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Values')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_with_mean(global_super_store_df, 'Sales')\n",
    "plot_box_with_mean(global_super_store_df, 'Profit')\n",
    "plot_box_with_mean(global_super_store_df, 'Quantity')\n",
    "plot_box_with_mean(global_super_store_df, 'Discount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Violin plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_violin(df, column):\n",
    "    \"\"\"\n",
    "    Plot a violin plot for a specified column in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame.\n",
    "    column (str): Name of the column to plot.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Create a violin plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.violinplot(x=df[column])\n",
    "    plt.title(f'Violin plot of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Values')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_violin(global_super_store_df, 'Sales')\n",
    "plot_violin(global_super_store_df, 'Profit')\n",
    "plot_violin(global_super_store_df, 'Quantity')\n",
    "plot_violin(global_super_store_df, 'Discount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping the data on category and it's respective sub-categories. Calculating the profit margin.\n",
    "sales_per_cat_subcat = global_super_store_df.groupby(['Category', 'Sub-Category'], as_index=False)[['Sales', 'Profit']].sum()\n",
    "sales_per_cat_subcat['Profit Margin'] = sales_per_cat_subcat['Profit'] / sales_per_cat_subcat['Sales']\n",
    "\n",
    "#Sorting the dataframe based on profit margin\n",
    "sales_per_cat_subcat = sales_per_cat_subcat.sort_values(by=['Category', 'Sub-Category', 'Profit Margin'], ascending=True)\n",
    "\n",
    "# plot a profit margins sub-category bar chart \n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "# Unique sub categories without NO_SUB_CATEGORY\n",
    "unique_sub_categories_without_custom_label = sales_per_cat_subcat[sales_per_cat_subcat['Sub-Category'] != 'NO_SUB_CATEGORY']['Sub-Category'].unique()\n",
    "\n",
    "#Plotting the profit margin per sub-category.\n",
    "sns.barplot(y=sales_per_cat_subcat['Sub-Category'], x=sales_per_cat_subcat['Profit Margin'], hue=sales_per_cat_subcat['Category'], \n",
    "                alpha=1, dodge=False, ax=ax, order=unique_sub_categories_without_custom_label)\n",
    "\n",
    "#Cleaning out bar junk\n",
    "ax.spines['left'].set_position(('data', 0.0))\n",
    "ax.spines[['right','top']].set_visible(False)\n",
    "ax.set(ylabel=None, xlabel='Profit Margin (%)')\n",
    "\n",
    "def move_ylabel_tick(index: list):\n",
    "    \"\"\"\n",
    "    Moving the provided ylabel ticks\n",
    "    \"\"\"\n",
    "    for tick in index:\n",
    "        ax.get_yticklabels()[tick].set_x(0.02)\n",
    "        ax.get_yticklabels()[tick].set_horizontalalignment('right')\n",
    "\n",
    "#Moving the y-labels on sub-categories that are making a loss in order to prevent collision of the bar and the text.\n",
    "move_ylabel_tick([-1, -2, -3])\n",
    "\n",
    "#Annotating the profit margin amount for each bar.\n",
    "for p in ax.patches:\n",
    "    _, y = p.get_xy()\n",
    "    \n",
    "    ax.annotate(f'{p.get_width()*100 :.1f}%', (p.get_width() / 2, y + 0.45))\n",
    "    \n",
    "#Calculating Superstore's aggregate profit margin in order to compare it to each sub-category's profit margin\n",
    "mean_profit = sales_per_cat_subcat['Profit'].sum() / sales_per_cat_subcat['Sales'].sum()\n",
    "\n",
    "#Plotting a vertical line and annotating the Superstore's aggregate profit margin.\n",
    "ax.axvline(mean_profit, color='blue', label='Mean Profit, All Categories', alpha=0.75, ls='-.')\n",
    "\n",
    "#Setting the title and legend.\n",
    "ax.set_title('Profit Margin by Sub-Category', fontdict={'fontsize':16})\n",
    "ax.legend(loc=(1, 0.9))\n",
    "\n",
    "#Formatting the x-axis as %\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trend(df, numerical_column):\n",
    "    \"\"\"\n",
    "    plot_trend, generates a trend plot for a specified column in a DataFrame, showing the total value of the column over the years.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing the data.\n",
    "    - column (str): The name of the column for which the trend plot is to be generated.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter out rows with 'Order Year' greater than 0\n",
    "    trend_data = df[df['Order Year'] > 1970]\n",
    "    # Group by 'Order Year' and calculate the sum of the specified column\n",
    "    trend_data = trend_data.groupby('Order Year')[numerical_column].sum()\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "    # Plot the bar plot\n",
    "    bars = ax.bar(x=trend_data.index, height=trend_data.values, color='lightblue')\n",
    "\n",
    "    # Annotate each bar with its value\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate('$ {:,.2f}'.format(height),\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "    # Plot the line plot\n",
    "    ax.plot(trend_data.index, trend_data.values, color='orange')\n",
    "\n",
    "    # Turn off scientific notation for y-axis\n",
    "    ax.yaxis.get_major_formatter().set_scientific(False)\n",
    "\n",
    "    # Set title and labels\n",
    "    plt.title(f'{numerical_column} trend over the years', fontsize=22)\n",
    "    plt.ylabel(f'Total {numerical_column}', fontsize=14)\n",
    "    plt.xlabel('Year', fontsize=14)\n",
    "\n",
    "    # Convert 'Order Year' column to a list of integers\n",
    "    years = trend_data.index.tolist()\n",
    "\n",
    "    # Set x-ticks to only include specific years\n",
    "    plt.xticks(years, rotation=45)\n",
    "\n",
    "    # Add gridlines\n",
    "    plt.grid(axis='x')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function for sales\n",
    "plot_trend(global_super_store_df, 'Sales')\n",
    "\n",
    "# Call the function for profit\n",
    "plot_trend(global_super_store_df, 'Profit')\n",
    "\n",
    "# Call the function for discounts\n",
    "plot_trend(global_super_store_df, 'Discount')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select Numerical Features\n",
    "numerical_features = ['Sales', 'Quantity', 'Discount', 'Profit']\n",
    "global_super_store_numerial_data = global_super_store_df[numerical_features]\n",
    "\n",
    "# Create Correlation matrix\n",
    "correlation_matrix = global_super_store_numerial_data.corr()\n",
    "correlation_matrix\n",
    "\n",
    "# Plot correlation matrix as heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interquartile Range (IQR) Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_plot_outliers_iqr(df, numerical_feature):\n",
    "    \"\"\"\n",
    "    Plots a boxplot for the specified numerical feature and identifies outliers based on the interquartile range (IQR).\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The pandas DataFrame containing the data.\n",
    "        numerical_feature (str): The name of the numerical feature to analyze.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    q1 = df[numerical_feature].quantile(0.25)\n",
    "    q3 = df[numerical_feature].quantile(0.75)\n",
    "\n",
    "    iqr = q3 - q1\n",
    "    lower_limit = q1 - 1.5 * iqr\n",
    "    upper_limit = q3 + 1.5 * iqr\n",
    "\n",
    "    outliers_df = df[(df[numerical_feature] < lower_limit)|(df[numerical_feature] > upper_limit)]\n",
    "    print(f\"Outlier numerical feature: {numerical_feature}, Outlier Count: {outliers_df[numerical_feature].count()}\")\n",
    "    \n",
    "    numerical_column = df[numerical_feature]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(data=numerical_column)\n",
    "    plt.title(f'Boxplot of Outliers for {numerical_feature}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_plot_outliers_iqr(global_super_store_df, 'Sales')\n",
    "detect_plot_outliers_iqr(global_super_store_df, 'Quantity')\n",
    "detect_plot_outliers_iqr(global_super_store_df, 'Discount')\n",
    "detect_plot_outliers_iqr(global_super_store_df, 'Profit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Z-Score Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_zscore(df, numerical_feature, threshold=3):\n",
    "    \"\"\"\n",
    "    Detect outliers in a numerical feature of a DataFrame using the Z-Score method.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The pandas DataFrame containing the data.\n",
    "        numerical_feature (str): The name of the numerical feature to analyze.\n",
    "        threshold (float): The Z-Score threshold for identifying outliers. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    data = df[numerical_feature]\n",
    "    z_scores = ((data - data.mean()) / data.std()).abs()\n",
    "    outliers = z_scores > threshold\n",
    "    outlier_count = outliers.sum()\n",
    "    print(f\"Outlier count for {numerical_feature}: {outlier_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_outliers_zscore(global_super_store_df, 'Sales')\n",
    "detect_outliers_zscore(global_super_store_df, 'Quantity')\n",
    "detect_outliers_zscore(global_super_store_df, 'Discount')\n",
    "detect_outliers_zscore(global_super_store_df, 'Profit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conculsion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions to plot bivariate analysis and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function: categorical_analysis() uses cross-tabulation to plot heatmaps/stacked bar plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_analysis(df, cat_var1, cat_var2):\n",
    "    \"\"\"\n",
    "    Perform bivariate analysis and visualization between two categorical variables.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): Input DataFrame containing the data.\n",
    "        cat_var1 (str): Name of the first categorical variable.\n",
    "        cat_var2 (str): Name of the second categorical variable.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cross-tabulation\n",
    "    cross_tab = pd.crosstab(df[cat_var1], df[cat_var2])\n",
    "\n",
    "    # Visualize cross-tabulation using a heatmap\n",
    "    plt.figure(figsize=(11, 6))\n",
    "    sns.heatmap(cross_tab, annot=True, fmt='d', cmap='YlGnBu')\n",
    "    plt.title(f'Cross-tabulation between {cat_var1} and {cat_var2}')\n",
    "    plt.xlabel(cat_var2)\n",
    "    plt.ylabel(cat_var1)\n",
    "    plt.show()\n",
    "\n",
    "    # Stacked bar plot for better visualization\n",
    "    cross_tab.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "    plt.title(f'Stacked Bar Plot of {cat_var1} by {cat_var2}')\n",
    "    plt.xlabel(cat_var1)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title=cat_var2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function: bivariate_categorical_sales_figure_analysis() uses pivot tables to plot heatmaps/stacked bar plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bivariate_categorical_sales_figure_analysis(df, cat_var1, cat_var2, numerical_feature):\n",
    "    \"\"\"\n",
    "    Perform bivariate analysis and visualization between two categorical variables based on a numerical feature.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): Input DataFrame containing the data.\n",
    "        cat_var1 (str): Name of the first categorical variable.\n",
    "        cat_var2 (str): Name of the second categorical variable.\n",
    "    \"\"\"\n",
    "    # Create a pivot table with 'Sales' values\n",
    "    pivot_table = df.pivot_table(values=numerical_feature, index=cat_var1, columns=cat_var2, aggfunc='sum', fill_value=0)\n",
    "    \n",
    "    # Define color palette\n",
    "    color_palette = sns.color_palette(\"YlGnBu\")\n",
    "    \n",
    "    # Plotting the pivot table\n",
    "    plt.figure(figsize=(10.5, 6))\n",
    "    sns.heatmap(pivot_table, annot=True, cmap=color_palette, fmt=',.2f')\n",
    "    plt.title(f'Cross-tabulation of Total {numerical_feature} between {cat_var1} and {cat_var2} in USD')\n",
    "    plt.xlabel(cat_var2)\n",
    "    plt.ylabel(cat_var1)\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting the pivot table as a stacked bar plot\n",
    "    ax = pivot_table.plot(kind='bar', stacked=True, figsize=(9, 6), color=color_palette)\n",
    "    ax.set_title(f'Stacked Bar Plot of Total {numerical_feature} by {cat_var1} and {cat_var2}')\n",
    "    ax.set_xlabel(cat_var1)\n",
    "    ax.set_ylabel(f'Total {numerical_feature}')\n",
    "    ax.set_xticklabels(pivot_table.index, rotation=45)\n",
    "    \n",
    "    # Format y-axis tick labels as currency\n",
    "    ax.yaxis.set_major_formatter('$ {:,.2f}'.format)\n",
    "    \n",
    "    ax.legend(title=cat_var2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segment Vs. Ship Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_analysis(global_super_store_df, 'Segment', 'Ship Mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bivariate_categorical_sales_figure_analysis(global_super_store_df, 'Segment', 'Ship Mode', 'Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bivariate_categorical_sales_figure_analysis(global_super_store_df, 'Segment', 'Ship Mode', 'Profit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bivariate_categorical_sales_figure_analysis(global_super_store_df, 'Segment', 'Ship Mode', 'Discount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical vs numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bivariate_numerical_analysis(df, num_var1, num_var2):\n",
    "    \"\"\"\n",
    "    Perform bivariate analysis and visualization between two numerical variables.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): Input DataFrame containing the data.\n",
    "        num_var1 (str): Name of the first numerical variable.\n",
    "        num_var2 (str): Name of the second numerical variable.\n",
    "    \"\"\"\n",
    "    # Scatter plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=num_var1, y=num_var2, data=df)\n",
    "    plt.title(f'Scatter plot of {num_var1} vs {num_var2}')\n",
    "    plt.xlabel(num_var1)\n",
    "    plt.ylabel(num_var2)\n",
    "    plt.show()\n",
    "    \n",
    "    # Correlation matrix\n",
    "    corr_matrix = df[[num_var1, num_var2]].corr()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bivariate_numerical_analysis(global_super_store_df, 'Sales', 'Quantity')\n",
    "bivariate_numerical_analysis(global_super_store_df, 'Sales', 'Profit')\n",
    "bivariate_numerical_analysis(global_super_store_df, 'Sales', 'Discount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical vs numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Profit Margin by Sub-categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geo Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_analysis_by_state(df, feature_column):\n",
    "    \"\"\"\n",
    "    geo_analysis_by_state, plots the feature percentages on a US map grouped by States\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): DataFrame containing the feature data.\n",
    "    - feature_column (str): Name of the numerical feature column.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    \n",
    "    base_folder_path = os.getcwd()\n",
    "    shp_path = '/data/usa-states-census-2014.shp'\n",
    "\n",
    "    us_states = gpd.read_file(f'{base_folder_path}{shp_path}')\n",
    "    us_states = us_states.to_crs(\"EPSG:3395\")\n",
    "\n",
    "    # Group feature data based on States\n",
    "    feature_per_states = df.groupby(['State'], as_index=False)[[feature_column]].sum()\n",
    "\n",
    "    # Create a new column to see the Feature %\n",
    "    feature_percent_column_name = feature_column + ' %'\n",
    "    feature_per_states[feature_percent_column_name] = (feature_per_states[feature_column] / df[feature_column].sum()) * 100\n",
    "\n",
    "    # Merge sales data with the US map based on state codes or names\n",
    "    merged_data = us_states.merge(feature_per_states, how='left', left_on='NAME', right_on='State')\n",
    "\n",
    "    # Filter out duplicate entries\n",
    "    merged_data = merged_data.drop_duplicates(subset=['NAME'])\n",
    "\n",
    "    # Plot the map\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(30, 10))\n",
    "    us_states.plot(ax=ax, color='lightgrey', edgecolor='black')\n",
    "    merged_data.plot(column=feature_percent_column_name, cmap='Spectral_r', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True, aspect='equal')\n",
    "\n",
    "    # Annotate state names and sales percentages\n",
    "    texts = []\n",
    "    for idx, row in merged_data.iterrows():\n",
    "        x = row['geometry'].centroid.x\n",
    "        y = row['geometry'].centroid.y\n",
    "        name = row['NAME']\n",
    "        feature_percent = row[feature_percent_column_name]\n",
    "        text = ax.text(x, y, name, fontsize=9, ha='center', va='center', color='black')\n",
    "        texts.append(text)\n",
    "        text = ax.text(x, y-0.5, f\"{feature_percent:.2f}%\", fontsize=9, ha='center', va='center', color='black')  # Adjust y position\n",
    "        texts.append(text)\n",
    "\n",
    "    # Adjust text labels to avoid overlaps\n",
    "    adjust_text(texts, arrowprops=dict(arrowstyle='-', color='grey'))\n",
    "\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'{feature_column} percent by US State', loc='center', fontsize=20, y=1.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sales % based on US States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_analysis_by_state(global_super_store_df, 'Sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Profit % based on US States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_analysis_by_state(global_super_store_df, 'Profit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discount % based on US States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_analysis_by_state(global_super_store_df, 'Discount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Geo Analysis Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sales conclusion:\n",
    "    * **California** has the most percentage of Sales with ~20%.\n",
    "    * The Central and South regions of the country collectively could not match the sales numbers of California.\n",
    "\n",
    "* Profits conclusion:\n",
    "    * **California** and **New York** are nearly tied with ~25% of the Profits. **Texas** despite of making ~7% of Sales, has grossed an ~8% of losses.\n",
    "\n",
    "* Discounts conclusion:\n",
    "    * **Texas** has received nearly ~24% discounts in Total. This coincides with the ~8% of the losses that the company made."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
