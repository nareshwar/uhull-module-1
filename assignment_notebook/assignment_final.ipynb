{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOBAL SUPERSTORE DATASET SALES ANALYSIS\n",
    "By Raju Vaneshwar Nareshwar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load first 10 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install packages\n",
    "import sys\n",
    "!{sys.executable} -m pip install geopandas\n",
    "!{sys.executable} -m pip install adjustText\n",
    "\n",
    "# import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import socket\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "\n",
    "from matplotlib import ticker as mtick\n",
    "from tabulate import tabulate\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# Letting pandas to show max columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV file and assigning into a dataframe ss_data\n",
    "global_super_store_df_org = pd.read_csv('sample-superstore-2023-T3.csv')\n",
    "\n",
    "# Copy the dataframe before processing\n",
    "global_super_store_df = global_super_store_df_org.copy();\n",
    "\n",
    "# Set the head to 10 to retrieve the first 10 records\n",
    "first_10_rows = global_super_store_df.head(n=10)\n",
    "first_10_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information of descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using info() and describe() function to get the descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metadata information about the dataset\n",
    "# Grouping data types by category\n",
    "numerical_columns = global_super_store_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_columns = global_super_store_df.select_dtypes(include=['object']).columns\n",
    "datetime_columns = global_super_store_df.select_dtypes(include=['datetime64']).columns\n",
    "\n",
    "# Define a table width and print the header row with a dotted line\n",
    "table_width = 215\n",
    "print(\"-\" * table_width)\n",
    "\n",
    "# Print each row of the table with content and a dash line\n",
    "content_list = [f\"We are working with a {global_super_store_df.shape} sized dataset.\", \n",
    "                    f\"Numercial columns: {', '.join(numerical_columns)}\",\n",
    "                    f\"Categorical columns: {', '.join(categorical_columns)}\",\n",
    "                    f\"Date Time columns: {', '.join(datetime_columns)}\"]\n",
    "\n",
    "for content_row in content_list:\n",
    "    print(\"| {:<211} |\".format(content_row))\n",
    "    print(\"-\" * table_width)\n",
    "\n",
    "# Get descriptive statistics on the dataset\n",
    "description_table = tabulate(global_super_store_df.describe(), headers='keys', tablefmt='pretty')\n",
    "\n",
    "# Print the table\n",
    "print(description_table)\n",
    "\n",
    "# Print metatype information about the dataset\n",
    "print(global_super_store_df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary key of these records are a system-generated, and denoted as column: *RowID*\n",
    "\n",
    "The datatypes of the dataset are following:\n",
    "* int64(1)\n",
    "* float64(2)\n",
    "* object(18)\n",
    "\n",
    "A few records of *Quantity* and *Profit* columns has the datatype of object, but it must be float64, thus needs to be cleansed or transformed.  \n",
    "\n",
    "*Ship Date* and *Order Date* columns are represented as strings, those needs to be converted as datetime.\n",
    "\n",
    "Once cleansed, the descriptive statistics can be applied to the numerial columns, and they are Sales, Quantity, Discount and Profit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms based on categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(data, xlabel, ylabel, title): \n",
    "    \n",
    "    \"\"\"    \n",
    "    plot_histogram generates a histogram plot for the given dataset.\n",
    "    Parameters:\n",
    "      data: Dataset for the histogram plot (pandas Series or any iterable).\n",
    "      xlabel: Label for the x-axis.\n",
    "      ylabel: Label for the y-axis.\n",
    "      title: Title of the plot.\n",
    "    Returns: None\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    bars = plt.bar(data.index, data)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, height, height, ha='center', va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for 'Ship Mode'\n",
    "orders_per_segment = global_super_store_df['Ship Mode'].value_counts()\n",
    "plot_histogram(orders_per_segment, 'Ship Mode', 'Number of Orders', 'Number of Orders per Ship Mode')\n",
    "\n",
    "# Histogram for 'Segment'\n",
    "orders_per_segment = global_super_store_df['Segment'].value_counts()\n",
    "plot_histogram(orders_per_segment, 'Segment', 'Number of Orders', 'Number of Orders per Segment')\n",
    "\n",
    "# Histogram for 'Category'\n",
    "orders_per_category = global_super_store_df['Category'].value_counts()\n",
    "plot_histogram(orders_per_category, 'Category', 'Number of Orders', 'Number of Orders per Category')\n",
    "\n",
    "# Histogram for 'Sub-Category'\n",
    "orders_per_sub_category = global_super_store_df['Sub-Category'].value_counts()\n",
    "plot_histogram(orders_per_sub_category, 'Sub-Category', 'Number of Orders', 'Number of Orders per Sub-Category')\n",
    "\n",
    "# Histogram for 'Region'\n",
    "orders_per_sub_category = global_super_store_df['Region'].value_counts()\n",
    "plot_histogram(orders_per_sub_category, 'Region', 'Number of Orders', 'Number of Orders per Region')\n",
    "\n",
    "# Histogram for 'State'\n",
    "orders_per_sub_category = global_super_store_df['State'].value_counts()\n",
    "plot_histogram(orders_per_sub_category, 'State', 'Number of Orders', 'Number of Orders per State')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **plot_histogram()** function can be run after the data cleansing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing values in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing any statistical analysis, the numerical column data has to be cleansed to be meaningful.\n",
    "* Records with special characters on *Quantity* and needs to be cleansed. \n",
    "* Records with special characters on *Profit* and needs to be cleansed. \n",
    "* Applying the **text2float()** function to fix *Quantity* column. \n",
    "* City and States are missing on a few records, **get_city_from_postal_code()** and **get_state_from_postal_code()** functions will fix them via API.\n",
    "* *Category/Sub-Category* needs cleansing and reorganizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2float(textnum, numwords={}):\n",
    "    \"\"\"\n",
    "    text2float converts the textual representation of numbers to float.\n",
    "    Parameters:\n",
    "        textnum: Textual representation of a number.\n",
    "        numwords: Dictionary mapping words to their numeric values.\n",
    "    Returns:\n",
    "        Float: The numerical value represented by the input text.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Attempt to convert to float\n",
    "        return float(textnum)\n",
    "    except ValueError:\n",
    "        # If conversion to float fails, continue with text to number conversion\n",
    "        textnum = textnum.lower()\n",
    "        \n",
    "        if not numwords:\n",
    "            units = [\n",
    "                \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
    "                \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "                \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\",\n",
    "            ]\n",
    "\n",
    "            tens = [\"\", \"\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"]\n",
    "\n",
    "            scales = [\"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n",
    "\n",
    "            numwords[\"and\"] = (1, 0)\n",
    "            for idx, word in enumerate(units):\n",
    "                numwords[word] = (1, idx)\n",
    "            for idx, word in enumerate(tens):\n",
    "                numwords[word] = (1, idx * 10)\n",
    "            for idx, word in enumerate(scales):\n",
    "                numwords[word] = (10 ** (idx * 3 or 2), 0)\n",
    "\n",
    "        current = result = 0\n",
    "        for word in textnum.split():\n",
    "            if word not in numwords:\n",
    "                raise Exception(\"Illegal word: \" + word)\n",
    "\n",
    "            scale, increment = numwords[word]\n",
    "            current = current * scale + increment\n",
    "            if scale > 100:\n",
    "                result += current\n",
    "                current = 0\n",
    "\n",
    "        return result + current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_internet_connected():\n",
    "    \"\"\"\n",
    "    Check if the machine is connected to the internet by attempting to connect to Google's DNS server.\n",
    "    Returns:\n",
    "        bool: True if the machine is connected to the internet, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to connect to Google's DNS server\n",
    "        socket.create_connection((\"8.8.8.8\", 53), timeout=3)\n",
    "        return True\n",
    "    except OSError:\n",
    "        pass\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_from_postal_code(postal_code):\n",
    "    \"\"\"\n",
    "    get_city_from_postal_code retrieves the city name from postal code using zippopotam API.\n",
    "    Parameters:\n",
    "        postal_code: Postal code for the city.\n",
    "    Returns:\n",
    "        str or None: City name corresponding to the given postal code, or None if postal_code is empty or invalid.\n",
    "    \"\"\"\n",
    "    if not is_internet_connected():\n",
    "        return None\n",
    "\n",
    "    if postal_code == '':\n",
    "        return None\n",
    "\n",
    "    url = f\"http://api.zippopotam.us/us/{postal_code}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        city = data['places'][0]['place name']\n",
    "        return city\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_from_postal_code(postal_code):\n",
    "    \"\"\"\n",
    "    get_state_from_postal_code retrieves the state name from postal code using zippopotam API.\n",
    "    Parameters:\n",
    "        postal_code: Postal code for the state.\n",
    "    Returns:\n",
    "        str or None: State name corresponding to the given postal code, or None if postal_code is empty or invalid.\n",
    "    \"\"\"\n",
    "\n",
    "    if not is_internet_connected():\n",
    "        return None\n",
    "\n",
    "    if postal_code == '':\n",
    "        return None\n",
    "\n",
    "    url = f\"http://api.zippopotam.us/us/{postal_code}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        state = data['places'][0]['state']\n",
    "        return state\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row ID is not needed for the analysis, hence dropping the column\n",
    "if 'Row ID' in global_super_store_df.columns:\n",
    "    global_super_store_df.drop('Row ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing \"?\" from Quantity column\n",
    "global_super_store_df['Quantity'] = global_super_store_df['Quantity'].str.replace('?', '')\n",
    "\n",
    "# Removing \"\"\" from Profit column\n",
    "global_super_store_df['Profit'] = global_super_store_df['Profit'].str.replace('\"', '')\n",
    "\n",
    "# Assuming zero values for NaN on Profits\n",
    "global_super_store_df['Profit'] = global_super_store_df['Profit'].fillna(0)\n",
    "\n",
    "# Removing \"\"\" from Postal Code column\n",
    "global_super_store_df['Postal Code'] = global_super_store_df['Postal Code'].str.replace('\"', '')\n",
    "\n",
    "# Make all records as Country = United States\n",
    "global_super_store_df['Country'] = 'United States'\n",
    "\n",
    "# Correcting spelling mistakes on Category column\n",
    "global_super_store_df['Category'] = global_super_store_df['Category'].replace('Frnture', 'Furniture')\n",
    "\n",
    "# Filling values on empty Category/Sub-Category records\n",
    "global_super_store_df['Category'] = global_super_store_df['Category'].fillna('NO_CATEGORY')\n",
    "global_super_store_df['Sub-Category'] = global_super_store_df['Sub-Category'].fillna('NO_SUB_CATEGORY')\n",
    "\n",
    "# Datafix on Category based on subcategories\n",
    "# Apply the condition element-wise\n",
    "condition = (global_super_store_df['Category'] == 'NO_CATEGORY') & \\\n",
    "            (global_super_store_df['Sub-Category'].isin(['Binders', 'Storage']))\n",
    "\n",
    "# Update 'Category' where the condition is True\n",
    "global_super_store_df.loc[condition, 'Category'] = 'Office Supplies'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanse the Regions\n",
    "central_regions_to_replace = ['Centrl', 'Cntral']\n",
    "east_regions_to_replace = ['Est']\n",
    "south_regions_to_replace = ['Southh']\n",
    "\n",
    "global_super_store_df['Region'] = global_super_store_df['Region'].replace(central_regions_to_replace, 'Central')\n",
    "global_super_store_df['Region'] = global_super_store_df['Region'].replace(east_regions_to_replace, 'East')\n",
    "global_super_store_df['Region'] = global_super_store_df['Region'].replace(south_regions_to_replace, 'South')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying get_state_from_postal_code function\n",
    "\n",
    "# Filter the empty state rows\n",
    "state_filtered_na = global_super_store_df.loc[pd.isna(global_super_store_df['State'])]\n",
    "state_filtered_na\n",
    "\n",
    "# Apply the function to fill the missing value via API\n",
    "global_super_store_df.loc[pd.isna(global_super_store_df['State']), 'State'] = state_filtered_na['Postal Code'].apply(get_state_from_postal_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying get_city_from_postal_code function\n",
    "\n",
    "# Filter the empty city rows\n",
    "city_filtered_na = global_super_store_df.loc[pd.isna(global_super_store_df['City'])]\n",
    "city_filtered_na\n",
    "\n",
    "# Apply the function to fill the missing value via API\n",
    "global_super_store_df.loc[pd.isna(global_super_store_df['City']), 'City'] = city_filtered_na['Postal Code'].apply(get_city_from_postal_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying text2float function\n",
    "global_super_store_df['Quantity'] = global_super_store_df['Quantity'].apply(text2float)\n",
    "global_super_store_df['Profit'] = global_super_store_df['Profit'].apply(text2float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for 'Ship Mode'\n",
    "orders_per_segment = global_super_store_df['Ship Mode'].value_counts()\n",
    "plot_histogram(orders_per_segment, 'Ship Mode', 'Number of Orders', 'Number of Orders per Ship Mode')\n",
    "\n",
    "# Histogram for 'Segment'\n",
    "orders_per_segment = global_super_store_df['Segment'].value_counts()\n",
    "plot_histogram(orders_per_segment, 'Segment', 'Number of Orders', 'Number of Orders per Segment')\n",
    "\n",
    "# Histogram for 'Category'\n",
    "orders_per_category = global_super_store_df['Category'].value_counts()\n",
    "plot_histogram(orders_per_category, 'Category', 'Number of Orders', 'Number of Orders per Category')\n",
    "\n",
    "# Histogram for 'Sub-Category'\n",
    "orders_per_sub_category = global_super_store_df['Sub-Category'].value_counts()\n",
    "plot_histogram(orders_per_sub_category, 'Sub-Category', 'Number of Orders', 'Number of Orders per Sub-Category')\n",
    "\n",
    "# Histogram for 'Region'\n",
    "orders_per_sub_category = global_super_store_df['Region'].value_counts()\n",
    "plot_histogram(orders_per_sub_category, 'Region', 'Number of Orders', 'Number of Orders per Region')\n",
    "\n",
    "# Histogram for 'State'\n",
    "orders_per_sub_category = global_super_store_df['State'].value_counts()\n",
    "plot_histogram(orders_per_sub_category, 'State', 'Number of Orders', 'Number of Orders per State')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_super_store_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with missing data\n",
    "print(f\"Sum of null records:\\n{global_super_store_df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping Sales/Profits based on Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_unique_values_on_tabulate(df, column_name):\n",
    "    \"\"\"\n",
    "    print_unique_values_on_tabulate, prints unique values from a specified column of a DataFrame in a table format.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame from which unique values will be extracted.\n",
    "    - column_name (str): The name of the column from which unique values will be extracted.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Get unique values from the specified column\n",
    "    unique_values = df[column_name].unique()\n",
    "    \n",
    "    # Flatten unique values array\n",
    "    flat_unique_values_array = unique_values.flatten()\n",
    "    \n",
    "    # Convert flattened array to list of lists\n",
    "    data = [[item] for item in flat_unique_values_array]\n",
    "    \n",
    "    # Hardcoded header\n",
    "    header = [f'Unique {column_name.capitalize()}']\n",
    "    \n",
    "    # Print the table using tabulate\n",
    "    print(tabulate(data, headers=header, tablefmt=\"pretty\", stralign =\"left\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_unique_values_on_tabulate(global_super_store_df, 'Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sales_profit_by_feature(df, feature_column, category_column):\n",
    "    \"\"\"\n",
    "    print_sales_profit_by_feature, prints total sales/[rpfit] by category in a tabular format.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing the sales data.\n",
    "    - sales_column (str): The name of the column containing sales data.\n",
    "    - sales_category_column (str): The name of the column to group by.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Group total sales by category\n",
    "    sales_category = df.groupby(category_column)[feature_column].sum().round(4).reset_index().sort_values(by=feature_column, ascending=False)\n",
    "    \n",
    "    # Convert the grouped DataFrame to a list of lists\n",
    "    sales_category_list = sales_category.values.tolist()\n",
    "\n",
    "    # Print the table using tabulate\n",
    "    print(tabulate(sales_category_list, headers=[category_column, 'Total Sales'], tablefmt=\"pretty\", stralign =\"left\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total sales by category from the highest sale.\n",
    "print_sales_profit_by_feature(global_super_store_df, 'Sales', 'Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total sales by category from the highest sale.\n",
    "print_sales_profit_by_feature(global_super_store_df, 'Profit', 'Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total sales by category, only considering positive sales\n",
    "positive_sales_category = global_super_store_df.groupby('Category').filter(lambda x: x['Sales'].sum() > 0).groupby('Category')['Sales'].sum()\n",
    "\n",
    "# group total profit by category, only considering positive profits\n",
    "positive_profit_category = global_super_store_df.groupby('Category').filter(lambda x: x['Profit'].sum() > 0).groupby('Category')['Profit'].sum()\n",
    "\n",
    "# figure size\n",
    "plt.figure(figsize=(16,12));\n",
    "\n",
    "# left total sales pie chart\n",
    "plt.subplot(1,2,1); # 1 row, 2 columns, the 1st plot.\n",
    "plt.pie(positive_sales_category.values, labels=positive_sales_category.index, startangle=90, counterclock=False,\n",
    "        autopct=lambda p:f'{p:.1f}% \\n £{p * np.sum(positive_sales_category.values) / 100 :,.0f}', \n",
    "        wedgeprops={'linewidth': 1, 'edgecolor':'black', 'alpha':0.75});\n",
    "plt.axis('square');\n",
    "plt.title('Total Sales by Category',  fontdict={'fontsize':16});\n",
    "\n",
    "# right total profits pie chart\n",
    "plt.subplot(1,2,2); # 1 row, 2 columns, the 2nd plot\n",
    "plt.pie(positive_profit_category.values, labels=positive_profit_category.index, startangle=90, counterclock=False,\n",
    "        autopct=lambda p:f'{p:.1f}% \\n £{p * np.sum(positive_profit_category.values) / 100 :,.0f}',\n",
    "        wedgeprops={'linewidth': 1, 'edgecolor':'black', 'alpha':0.75});\n",
    "plt.axis('square');\n",
    "plt.title('Total Profit by Category', fontdict={'fontsize':16});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > _Total Sales on Categories_\n",
    "\n",
    "1. Technology at 36.4% \n",
    "2. Furniture at 32.3%\n",
    "3. Office Supplies at 31.3%\n",
    "\n",
    "Sales depict a near-perfect symmmetery on categories, with **Technology** winning with a slight edge.\n",
    "\n",
    " > _Total Profits on Categories_\n",
    "\n",
    "1. Technology at 50.1%\n",
    "2. Office Supplies at 42.7%\n",
    "3. Furniture at 7.2%\n",
    "\n",
    "Profits are largely taken by **Technology** category with *Office Supplies* being the lowest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping Sales/Profits based on Sub-Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_unique_values_on_tabulate(global_super_store_df, 'Sub-Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total sales by sub-category from the highest sale.\n",
    "print_sales_profit_by_feature(global_super_store_df, 'Sales', 'Sub-Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total profit by sub-category from the highest profit.\n",
    "print_sales_profit_by_feature(global_super_store_df, 'Profit', 'Sub-Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping the data on category and it's respective sub-categories. Calculating the profit margin.\n",
    "sales_per_cat_subcat = global_super_store_df.groupby(['Category', 'Sub-Category'], as_index=False)[['Sales', 'Profit']].sum()\n",
    "sales_per_cat_subcat['Profit Margin %'] = (sales_per_cat_subcat['Profit'] / sales_per_cat_subcat['Sales']) * 100\n",
    "\n",
    "#Sorting the dataframe based on profit margin\n",
    "sales_per_cat_subcat = sales_per_cat_subcat.sort_values(by=['Profit Margin %'], ascending=False)\n",
    "sales_per_cat_subcat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping Sales/Profits based on States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total sales by states from the highest sale.\n",
    "sales_by_states = global_super_store_df.groupby(['State'], as_index=False)['Sales'].sum().sort_values(by='Sales', ascending=False)\n",
    "sales_by_states['Sales %'] = (sales_by_states['Sales'] / global_super_store_df['Sales'].sum()) * 100\n",
    "print(sales_by_states.describe())\n",
    "sales_by_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **State which made the highest sales:** California\n",
    "> \n",
    "> **State which made the lowest sales:** North Dakota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping Sales/Profits based on Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total sales by segments from the highest sale.\n",
    "sales_by_segment = global_super_store_df.groupby(['Segment'], as_index=False)['Sales'].sum().sort_values(by='Sales', ascending=False)\n",
    "sales_by_segment['Sales %'] = (sales_by_segment['Sales'] / global_super_store_df['Sales'].sum()) * 100\n",
    "sales_by_segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consumer** Segment has 50% of Sales share, followed by **Corporate** and **Home Office**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping Sales/Profits based on Shipping Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total sales by segments from the highest sale.\n",
    "sales_by_ship_mode = global_super_store_df.groupby(['Ship Mode'], as_index=False)['Sales'].sum().sort_values(by='Sales', ascending=False)\n",
    "sales_by_ship_mode['Sales %'] = (sales_by_ship_mode['Sales'] / global_super_store_df['Sales'].sum()) * 100\n",
    "sales_by_ship_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard** shipping method is preferred as the sales percentage is nearly 60%, followed by **Second Class** and **First Class**. \n",
    "**Same Day** is not an economical option, so only preferred by 5% of the orders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the Order Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_super_store_df['Order Date'] = global_super_store_df['Order Date'].str.replace('$April', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_super_store_df['Order Date'] = pd.to_datetime(global_super_store_df['Order Date'], format='%d/%m/%Y')\n",
    "global_super_store_df['Ship Date'] = pd.to_datetime(global_super_store_df['Ship Date'], format='%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_super_store_df['Shipment Days'] = (global_super_store_df['Ship Date'] - global_super_store_df['Order Date']).dt.days\n",
    "sales_by_ship_mode_days = global_super_store_df.groupby(['Ship Mode'], as_index=False)['Shipment Days'].sum().sort_values(by='Shipment Days', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_month_and_date(ship_date):\n",
    "    ship_date_swapped_as_str = dt.datetime.strftime(ship_date, '%Y-%d-%m %H:%M:%S')\n",
    "    return pd.to_datetime(ship_date_swapped_as_str, format='%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumption:\n",
    "# All ship modes must deliver within 30 days\n",
    "# if it is more than 30 days, then it is not acceptable\n",
    "acceptable_shipment_days = np.arange(0, 31)\n",
    "\n",
    "# Get the all the records out of that filter to fix\n",
    "ship_date_fix_condition = (~global_super_store_df['Shipment Days'].isin(acceptable_shipment_days)) & \\\n",
    "                            (~global_super_store_df['Ship Date'].isna()) & \\\n",
    "                            (~global_super_store_df['Order Date'].isna())\n",
    "\n",
    "# Apply a function to swap the date and month\n",
    "global_super_store_df.loc[ship_date_fix_condition, 'Ship Date'] = global_super_store_df.loc[ship_date_fix_condition, 'Ship Date'].apply(swap_month_and_date)\n",
    "\n",
    "\n",
    "    # lambda x: pd.to_datetime(dt.datetime.strftime(x, '%Y-%d-%m %H:%M:%S')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_super_store_df['Shipment Days'] = (global_super_store_df['Ship Date'] - global_super_store_df['Order Date']).dt.days\n",
    "global_super_store_df.to_csv('01052024.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems with fixing the dates:\n",
    "\n",
    "1. We cannot take meaningful stats on the Order/Ship Dates\n",
    "2. A few records are in dd-mm-yyyy and a few of them are in mm-dd-yyyy\n",
    "3. Worst case in many scenarios, the Order Date is in dd-mm-yyyy and Ship Date is in mm-dd-yyyy and vice versa.\n",
    "4. The best we can do it see an year-on-year time series analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate analysis and visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Profit Margin by Sub-categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping the data on category and it's respective sub-categories. Calculating the profit margin.\n",
    "sales_per_cat_subcat = global_super_store_df.groupby(['Category', 'Sub-Category'], as_index=False)[['Sales', 'Profit']].sum()\n",
    "sales_per_cat_subcat['Profit Margin'] = sales_per_cat_subcat['Profit'] / sales_per_cat_subcat['Sales']\n",
    "\n",
    "#Sorting the dataframe based on profit margin\n",
    "sales_per_cat_subcat = sales_per_cat_subcat.sort_values(by=['Category', 'Sub-Category', 'Profit Margin'], ascending=True)\n",
    "\n",
    "# plot a profit margins sub-category bar chart \n",
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "\n",
    "# Unique sub categories without NO_SUB_CATEGORY\n",
    "unique_sub_categories_without_custom_label = sales_per_cat_subcat[sales_per_cat_subcat['Sub-Category'] != 'NO_SUB_CATEGORY']['Sub-Category'].unique()\n",
    "\n",
    "#Plotting the profit margin per sub-category.\n",
    "sns.barplot(y=sales_per_cat_subcat['Sub-Category'], x=sales_per_cat_subcat['Profit Margin'], hue=sales_per_cat_subcat['Category'], \n",
    "                alpha=1, dodge=False, ax=ax, order=unique_sub_categories_without_custom_label)\n",
    "\n",
    "#Cleaning out bar junk\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines[['right','top']].set_visible(False)\n",
    "ax.set(ylabel=None, xlabel='Profit Margin (%)')\n",
    "\n",
    "def move_ylabel_tick(index: list):\n",
    "    \"\"\"\n",
    "    Moving the provided ylabel ticks\n",
    "    \"\"\"\n",
    "    for tick in index:\n",
    "        ax.get_yticklabels()[tick].set_x(0.02)\n",
    "        ax.get_yticklabels()[tick].set_horizontalalignment('right')\n",
    "\n",
    "#Moving the y-labels on sub-categories that are making a loss in order to prevent collision of the bar and the text.\n",
    "move_ylabel_tick([-1, -2, -3])\n",
    "\n",
    "#Annotating the profit margin amount for each bar.\n",
    "for p in ax.patches:\n",
    "    _, y = p.get_xy()\n",
    "    \n",
    "    ax.annotate(f'{p.get_width()*100 :.1f}%', (p.get_width() / 2, y + 0.45))\n",
    "    \n",
    "#Calculating Superstore's aggregate profit margin in order to compare it to each sub-category's profit margin\n",
    "mean_profit = sales_per_cat_subcat['Profit'].sum() / sales_per_cat_subcat['Sales'].sum()\n",
    "\n",
    "#Plotting a vertical line and annotating the Superstore's aggregate profit margin.\n",
    "ax.axvline(mean_profit, color='blue', label='Mean Profit, All Categories', alpha=0.75, ls='-.')\n",
    "\n",
    "#Setting the title and legend.\n",
    "ax.set_title('Profit Margin by Sub-Category', fontdict={'fontsize':16})\n",
    "ax.legend(loc=(1, 0.9))\n",
    "\n",
    "#Formatting the x-axis as %\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geo Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_analysis_by_state(df, feature_column):\n",
    "    base_folder_path = os.getcwd()\n",
    "    shp_path = '/data/usa-states-census-2014.shp'\n",
    "\n",
    "    us_states = gpd.read_file(f'{base_folder_path}{shp_path}')\n",
    "    us_states = us_states.to_crs(\"EPSG:3395\")\n",
    "\n",
    "    # Group feature data based on States\n",
    "    feature_per_states = df.groupby(['State'], as_index=False)[[feature_column]].sum()\n",
    "\n",
    "    # Create a new column to see the Feature %\n",
    "    feature_percent_column_name = feature_column + ' %'\n",
    "    feature_per_states[feature_percent_column_name] = (feature_per_states[feature_column] / df[feature_column].sum()) * 100\n",
    "\n",
    "    # Merge sales data with the US map based on state codes or names\n",
    "    merged_data = us_states.merge(feature_per_states, how='left', left_on='NAME', right_on='State')\n",
    "\n",
    "    # Filter out duplicate entries\n",
    "    merged_data = merged_data.drop_duplicates(subset=['NAME'])\n",
    "\n",
    "    # Plot the map\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(30, 10))\n",
    "    us_states.plot(ax=ax, color='lightgrey', edgecolor='black')\n",
    "    merged_data.plot(column=feature_percent_column_name, cmap='Spectral_r', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True, aspect='equal')\n",
    "\n",
    "    # Annotate state names and sales percentages\n",
    "    texts = []\n",
    "    for idx, row in merged_data.iterrows():\n",
    "        x = row['geometry'].centroid.x\n",
    "        y = row['geometry'].centroid.y\n",
    "        name = row['NAME']\n",
    "        feature_percent = row[feature_percent_column_name]\n",
    "        text = ax.text(x, y, name, fontsize=9, ha='center', va='center', color='black')\n",
    "        texts.append(text)\n",
    "        text = ax.text(x, y-0.5, f\"{feature_percent:.2f}%\", fontsize=9, ha='center', va='center', color='black')  # Adjust y position\n",
    "        texts.append(text)\n",
    "\n",
    "    # Adjust text labels to avoid overlaps\n",
    "    adjust_text(texts, arrowprops=dict(arrowstyle='-', color='grey'))\n",
    "\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'{feature_column} percent by US State', loc='center', fontsize=20, y=1.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_analysis_by_state(global_super_store_df, 'Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_analysis_by_state(global_super_store_df, 'Profit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder_path = os.getcwd()\n",
    "shp_path = '/data/usa-states-census-2014.shp'\n",
    "\n",
    "us_states = gpd.read_file(f'{base_folder_path}{shp_path}')\n",
    "us_states = us_states.to_crs(\"EPSG:3395\")\n",
    "\n",
    "# Group data based on States\n",
    "sales_per_states = global_super_store_df.groupby(['State'], as_index=False)[['Sales']].sum()\n",
    "\n",
    "# Create a new column to see the Sales %\n",
    "sales_per_states['Sales %'] = (sales_per_states['Sales'] / global_super_store_df['Sales'].sum()) * 100\n",
    "\n",
    "# Merge sales data with the US map based on state codes or names\n",
    "merged_data = us_states.merge(sales_per_states, how='left', left_on='NAME', right_on='State')\n",
    "\n",
    "# Filter out duplicate entries\n",
    "merged_data = merged_data.drop_duplicates(subset=['NAME'])\n",
    "\n",
    "# Plot the map\n",
    "fig, ax = plt.subplots(1, 1, figsize=(30, 10))\n",
    "us_states.plot(ax=ax, color='lightgrey', edgecolor='black')\n",
    "merged_data.plot(column='Sales %', cmap='Spectral_r', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True, aspect='equal')\n",
    "\n",
    "# Annotate state names and sales percentages\n",
    "texts = []\n",
    "for idx, row in merged_data.iterrows():\n",
    "    x = row['geometry'].centroid.x\n",
    "    y = row['geometry'].centroid.y\n",
    "    name = row['NAME']\n",
    "    sales_percent = row['Sales %']\n",
    "    text = ax.text(x, y, name, fontsize=9, ha='center', va='center', color='black')\n",
    "    texts.append(text)\n",
    "    text = ax.text(x, y-0.5, f\"{sales_percent:.2f}%\", fontsize=9, ha='center', va='center', color='black')  # Adjust y position\n",
    "    texts.append(text)\n",
    "\n",
    "# Adjust text labels to avoid overlaps\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle='-', color='grey'))\n",
    "\n",
    "ax.axis('off')\n",
    "ax.set_title('Sales percent by US State', loc='center', fontsize=20, y=1.0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time series analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select Numerical Features\n",
    "numerical_features = ['Sales', 'Quantity', 'Discount', 'Profit']\n",
    "global_super_store_numerial_data = global_super_store_df[numerical_features]\n",
    "\n",
    "correlation_matrix = global_super_store_numerial_data.corr()\n",
    "correlation_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot correlation matrix as heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interquartile Range (IQR) Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['Sales', 'Quantity', 'Discount', 'Profit']\n",
    "\n",
    "for numerical_feature in numerical_features:\n",
    "    q1 = global_super_store_df[numerical_feature].quantile(0.25)\n",
    "    q3 = global_super_store_df[numerical_feature].quantile(0.75)\n",
    "    \n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    lower_limit = q1 - 1.5 * iqr\n",
    "    upper_limit = q3 + 1.5* iqr\n",
    "\n",
    "    outliers_df = global_super_store_df[(global_super_store_df[numerical_feature] < lower_limit)|(global_super_store_df[numerical_feature] > upper_limit)]\n",
    "    sns.boxplot(outliers_df)\n",
    "    print(f\"Outlier numerical feature: {numerical_feature}, Outlier Count: {outliers_df[numerical_feature].count()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
