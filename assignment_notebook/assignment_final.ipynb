{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOBAL SUPERSTORE DATASET SALES ANALYSIS\n",
    "By Raju Vaneshwar Nareshwar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Task 1](#task1)\n",
    "\n",
    "    1.1 [Load first 10 records](#load-first-10-records)\n",
    "    \n",
    "    1.2 [Understanding the dataset](#understanding-dataset)\n",
    "\n",
    "2. [Assess the data](#assess)\n",
    "\n",
    "    2.1 [Meta Data](#metadata)\n",
    "    \n",
    "    2.2 [Assessment Summary](#summary)\n",
    "\n",
    "3. [Data Cleaning](#clean)\n",
    "\n",
    "4. [Analysis and Data Visualization](#analysis)\n",
    "\n",
    "    4.1 [Product Analysis](#product)\n",
    "\n",
    "    4.2 [Segment Analysis](#segment)\n",
    "\n",
    "    4.3 [Geographical market location Analysis](#market)\n",
    "\n",
    "    4.4 [Shipping](#shipping)\n",
    "\n",
    "    4.5 [Time Series Analysis](#time)\n",
    "\n",
    "5. [Insights](#insights)\n",
    "\n",
    "    5.1 [Findings](#findings)\n",
    "\n",
    "    5.2 [Limitations](#limitation)\n",
    "\n",
    "    5.3 [Recommendations](#recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Task 1  <a id='task1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load first 10 records <a id='load-first-10-records'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker as mtick\n",
    "import seaborn as sns\n",
    "\n",
    "# Letting pandas to show max columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV file and assigning into a dataframe ss_data\n",
    "global_super_store_data = pd.read_csv('sample-superstore-2023-T3.csv')\n",
    "\n",
    "# Set the head to 10 to retrieve the first 10 records\n",
    "first_10_rows = global_super_store_data.head(n=10)\n",
    "first_10_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Understanding of the dataset <a id='understanding-dataset'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using info() and describe() function to get the descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metadata information about the dataset\n",
    "global_super_store_data.info()\n",
    "\n",
    "# Get descriptive statistics on the dataset\n",
    "global_super_store_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary key of these records are a system-generated, and denoted as column: *RowID*\n",
    "\n",
    "The datatypes of the dataset are following:\n",
    "* int64(1)\n",
    "* float64(2)\n",
    "* object(18)\n",
    "\n",
    "A few records of *Quantity* and *Profit* columns has the datatype of object, but it must be float64, thus needs to be cleansed or transformed.  \n",
    "*Ship Date* and *Order Date* columns are represented as strings, those needs to be converted as datetime.\n",
    "\n",
    "Once cleansed, the descriptive statistics can be applied to the numerial columns, and they are Sales, Quantity, Discount and Profit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function **text2float()** will take a txt number as a parameter and convert back to float64 number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2float(textnum, numwords={}):\n",
    "    try:\n",
    "        # Attempt to convert to float\n",
    "        return float(textnum)\n",
    "    except ValueError:\n",
    "        # If conversion to float fails, continue with text to number conversion\n",
    "        textnum = textnum.lower()\n",
    "        \n",
    "        if not numwords:\n",
    "            units = [\n",
    "                \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
    "                \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "                \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\",\n",
    "            ]\n",
    "\n",
    "            tens = [\"\", \"\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"]\n",
    "\n",
    "            scales = [\"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n",
    "\n",
    "            numwords[\"and\"] = (1, 0)\n",
    "            for idx, word in enumerate(units):\n",
    "                numwords[word] = (1, idx)\n",
    "            for idx, word in enumerate(tens):\n",
    "                numwords[word] = (1, idx * 10)\n",
    "            for idx, word in enumerate(scales):\n",
    "                numwords[word] = (10 ** (idx * 3 or 2), 0)\n",
    "\n",
    "        current = result = 0\n",
    "        for word in textnum.split():\n",
    "            if word not in numwords:\n",
    "                raise Exception(\"Illegal word: \" + word)\n",
    "\n",
    "            scale, increment = numwords[word]\n",
    "            current = current * scale + increment\n",
    "            if scale > 100:\n",
    "                result += current\n",
    "                current = 0\n",
    "\n",
    "        return result + current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing any statistical analysis, the numerical column data has to be cleansed to be meaningful.\n",
    "* Records with special characters on *Quantity* and needs to be cleansed. \n",
    "* Records with special characters on *Profit* and needs to be cleansed. \n",
    "* Applying the **text2float()** function to fix *Quantity* column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing \"?\" from Quantity column\n",
    "global_super_store_data['Quantity'] = global_super_store_data['Quantity'].str.replace('?', '')\n",
    "\n",
    "# Removing \"\"\" from Profit column\n",
    "global_super_store_data['Profit'] = global_super_store_data['Profit'].str.replace('\"', '')\n",
    "\n",
    "# Assuming zero values for NaN on Profits\n",
    "global_super_store_data['Profit'] = global_super_store_data['Profit'].fillna(0)\n",
    "\n",
    "\n",
    "# Removing \"\"\" from Postal Code column\n",
    "global_super_store_data['Postal Code'] = global_super_store_data['Postal Code'].str.replace('\"', '')\n",
    "\n",
    "# Applying text2float function\n",
    "global_super_store_data['Quantity'] = global_super_store_data['Quantity'].apply(text2float)\n",
    "global_super_store_data['Profit'] = global_super_store_data['Profit'].apply(text2float)\n",
    "global_super_store_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row ID is not needed for the analysis, hence dropping the column\n",
    "if 'Row ID' in global_super_store_data.columns:\n",
    "    global_super_store_data.drop('Row ID', axis=1, inplace=True)\n",
    "\n",
    "global_super_store_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with missing data\n",
    "print(f\"Sum of null records:\\n{global_super_store_data.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Outlier Treatment (Still needs to be worked on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_value = global_super_store_data['Sales'].mean()\n",
    "std_value = global_super_store_data['Sales'].std()\n",
    "\n",
    "# Define a threshold for identifying outliers (e.g., 3 standard deviations from the mean)\n",
    "threshold = 3\n",
    "lower_threshold = mean_value - threshold * std_value\n",
    "upper_threshold = mean_value + threshold * std_value\n",
    "\n",
    "print(f\"mean: {mean_value}\")\n",
    "print(f\"std: {std_value}\")\n",
    "print(f\"lower_threshold: {lower_threshold}\")\n",
    "print(f\"upper_threshold: {upper_threshold}\")\n",
    "\n",
    "\n",
    "# Filter outliers\n",
    "outliers = global_super_store_data[(global_super_store_data['Sales'] < lower_threshold) | (global_super_store_data['Sales'] > upper_threshold)]\n",
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Normalizing and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "#Select Numerical Features\n",
    "numerical_features = ['Sales', 'Quantity', 'Discount', 'Profit']\n",
    "global_super_store_numerial_data = global_super_store_data[numerical_features]\n",
    "\n",
    "# # Min-Max Scaling\n",
    "# scaler = MinMaxScaler()\n",
    "# scaled_data = scaler.fit_transform(global_super_store_numerial_data)\n",
    "\n",
    "# # Creating DataFrame with scaled data\n",
    "# scaled_df = pd.DataFrame(scaled_data, columns=global_super_store_numerial_data.columns)\n",
    "\n",
    "# print(f\"Min-Max Scaled DataFrame:\\n{scaled_df}\")\n",
    "\n",
    "# # Standard Scaling (Z-score normalization)\n",
    "# scaler = StandardScaler()\n",
    "# standardized_data = scaler.fit_transform(global_super_store_numerial_data)\n",
    "\n",
    "# # Creating DataFrame with standardized data\n",
    "# standardized_df = pd.DataFrame(standardized_data, columns=global_super_store_numerial_data.columns)\n",
    "\n",
    "# print(\"\\nStandardized DataFrame:\")\n",
    "# standardized_df\n",
    "\n",
    "\n",
    "# Correlation matrix\n",
    "correlation_matrix = global_super_store_numerial_data.corr()\n",
    "correlation_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Grouping of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping Sales/Porfits based on Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_categories = global_super_store_data['Category'].unique();\n",
    "unique_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "Technology         836154.0330\n",
       "Furniture          733206.6573\n",
       "Office Supplies    718808.1040\n",
       "Frnture              8621.0280\n",
       "Name: Sales, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group total sales by category from the highest sale.\n",
    "sales_category = global_super_store_data.groupby('Category')['Sales'].sum().sort_values(ascending=False)\n",
    "sales_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total profits by category\n",
    "profit_category = global_super_store_data.groupby('Category')['Profit'].sum().sort_values(ascending=False)\n",
    "profit_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total sales by category\n",
    "sales_category = global_super_store_data.groupby('Category')['Sales'].sum()\n",
    "\n",
    "# group total profits by category\n",
    "profit_category = global_super_store_data.groupby('Category')['Profit'].sum()\n",
    "\n",
    "\n",
    "# figure size\n",
    "plt.figure(figsize=(16,12));\n",
    "\n",
    "# left total sales pie chart\n",
    "plt.subplot(1,2,1); # 1 row, 2 columns, the 1st plot.\n",
    "plt.pie(sales_category.values, labels=sales_category.index, startangle=90, counterclock=False,\n",
    "        autopct=lambda p:f'{p:.1f}% \\n Â£{p*np.sum(sales_category.values)/100 :,.0f}', \n",
    "        wedgeprops={'linewidth': 1, 'edgecolor':'black', 'alpha':0.75});\n",
    "plt.axis('square');\n",
    "plt.title('Total Sales by Category',  fontdict={'fontsize':16});\n",
    "\n",
    "# right total profits pie chart\n",
    "plt.subplot(1,2,2); # 1 row, 2 columns, the 2nd plot\n",
    "plt.pie(profit_category.values, labels=profit_category.index, startangle=90, counterclock=False,\n",
    "        autopct=lambda p:f'{p:.1f}% \\n ${p*np.sum(profit_category.values)/100 :,.0f}',\n",
    "        wedgeprops={'linewidth': 1, 'edgecolor':'black', 'alpha':0.75});\n",
    "plt.axis('square');\n",
    "plt.title('Total Profit by Category', fontdict={'fontsize':16});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Handling missing values in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Correlation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Univariate analysis and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_describe['Profit'] = columns_to_describe['Profit'].str.replace('\"','')\n",
    "columns_to_describe['Profit'] = columns_to_describe['Profit'].fillna(0)\n",
    "columns_to_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_describe.describe()\n",
    "columns_to_describe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_describe['Profit'] = columns_to_describe['Profit'].apply(text2float)\n",
    "columns_to_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_describe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_describe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_describe['Transformed'] = np.log1p(columns_to_describe['Sales'])\n",
    "print(\"Original data:\")\n",
    "columns_to_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_data['Profit'] = ss_data['Profit'].str.replace('\"', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_data['Quantity'] = ss_data['Quantity'].str.replace('?', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_data['Quantity'] = ss_data['Quantity'].apply(text2float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_data['Postal Code'] = ss_data['Postal Code'].str.replace('\"', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_data['Country'] = ss_data['Country'].str.replace('US', 'United States')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_data.to_csv('10042024.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_data['State'] = ss_data['State'].dropna()\n",
    "us_states = ss_data['State'].unique()\n",
    "\n",
    "print(us_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(ss_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_data['Category'] = ss_data['Category'].str.replace('Frnture', 'Furniture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(ss_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_data.groupby('State').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=ss_data, x='Sales', y='Quantity', hue='Category')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
