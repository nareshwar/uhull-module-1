{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOBAL SUPERSTORE DATASET SALES ANALYSIS\n",
    "By Raju Vaneshwar Nareshwar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Task 1](#task1)\n",
    "\n",
    "    1.1 [Load first 10 records](#load-first-10-records)\n",
    "    \n",
    "    1.2 [Understanding the dataset](#understanding-dataset)\n",
    "\n",
    "2. [Assess the data](#assess)\n",
    "\n",
    "    2.1 [Meta Data](#metadata)\n",
    "    \n",
    "    2.2 [Assessment Summary](#summary)\n",
    "\n",
    "3. [Data Cleaning](#clean)\n",
    "\n",
    "4. [Analysis and Data Visualization](#analysis)\n",
    "\n",
    "    4.1 [Product Analysis](#product)\n",
    "\n",
    "    4.2 [Segment Analysis](#segment)\n",
    "\n",
    "    4.3 [Geographical market location Analysis](#market)\n",
    "\n",
    "    4.4 [Shipping](#shipping)\n",
    "\n",
    "    4.5 [Time Series Analysis](#time)\n",
    "\n",
    "5. [Insights](#insights)\n",
    "\n",
    "    5.1 [Findings](#findings)\n",
    "\n",
    "    5.2 [Limitations](#limitation)\n",
    "\n",
    "    5.3 [Recommendations](#recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Task 1  <a id='task1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load first 10 records <a id='load-first-10-records'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: geopandas in c:\\users\\nares\\appdata\\roaming\\python\\python311\\site-packages (0.14.3)\n",
      "Requirement already satisfied: fiona>=1.8.21 in c:\\users\\nares\\appdata\\roaming\\python\\python311\\site-packages (from geopandas) (1.9.6)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from geopandas) (23.1)\n",
      "Requirement already satisfied: pandas>=1.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from geopandas) (2.0.3)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in c:\\users\\nares\\appdata\\roaming\\python\\python311\\site-packages (from geopandas) (3.6.1)\n",
      "Requirement already satisfied: shapely>=1.8.0 in c:\\users\\nares\\appdata\\roaming\\python\\python311\\site-packages (from geopandas) (2.0.4)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from fiona>=1.8.21->geopandas) (22.1.0)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from fiona>=1.8.21->geopandas) (2024.2.2)\n",
      "Requirement already satisfied: click~=8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from fiona>=1.8.21->geopandas) (8.0.4)\n",
      "Requirement already satisfied: click-plugins>=1.0 in c:\\users\\nares\\appdata\\roaming\\python\\python311\\site-packages (from fiona>=1.8.21->geopandas) (1.1.1)\n",
      "Requirement already satisfied: cligj>=0.5 in c:\\users\\nares\\appdata\\roaming\\python\\python311\\site-packages (from fiona>=1.8.21->geopandas) (0.7.2)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from fiona>=1.8.21->geopandas) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.4.0->geopandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.4.0->geopandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.4.0->geopandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.4.0->geopandas) (1.24.3)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click~=8.0->fiona>=1.8.21->geopandas) (0.4.6)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting GDAL\n",
      "  Using cached GDAL-3.8.5.tar.gz (802 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: GDAL\n",
      "  Building wheel for GDAL (setup.py): started\n",
      "  Building wheel for GDAL (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for GDAL\n",
      "Failed to build GDAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [138 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-311\n",
      "      creating build\\lib.win-amd64-cpython-311\\osgeo\n",
      "      copying osgeo\\gdal.py -> build\\lib.win-amd64-cpython-311\\osgeo\n",
      "      copying osgeo\\gdalconst.py -> build\\lib.win-amd64-cpython-311\\osgeo\n",
      "      copying osgeo\\gdalnumeric.py -> build\\lib.win-amd64-cpython-311\\osgeo\n",
      "      copying osgeo\\gdal_array.py -> build\\lib.win-amd64-cpython-311\\osgeo\n",
      "      copying osgeo\\gnm.py -> build\\lib.win-amd64-cpython-311\\osgeo\n",
      "      copying osgeo\\ogr.py -> build\\lib.win-amd64-cpython-311\\osgeo\n",
      "      copying osgeo\\osr.py -> build\\lib.win-amd64-cpython-311\\osgeo\n",
      "      copying osgeo\\__init__.py -> build\\lib.win-amd64-cpython-311\\osgeo\n",
      "      creating build\\lib.win-amd64-cpython-311\\osgeo_utils\n",
      "      copying gdal-utils\\osgeo_utils\\gdal2tiles.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\n",
      "      copying gdal-utils\\osgeo_utils\\gdal2xyz.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\n",
      "      copying gdal-utils\\osgeo_utils\\gdalattachpct.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\n",
      "      copying gdal-utils\\osgeo_utils\\gdalcompare.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\n",
      "      copying gdal-utils\\osgeo_utils\\gdalmove.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\n",
      "      copying gdal-utils\\osgeo_utils\\gdal_calc.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\n",
      "      copying gdal-utils\\osgeo_utils\\gdal_edit.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\n",
      "      copying gdal-utils\\osgeo_utils\\gdal_fillnodata.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\n",
      "      copying gdal-utils\\osgeo_utils\\gdal_merge.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\n",
      "      copying gdal-utils\\osgeo_utils\\gdal_pansharpen.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\n",
      "      copying gdal-utils\\osgeo_utils\\gdal_polygonize.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\n",
      "      copying gdal-utils\\osgeo_utils\\gdal_proximity.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\n",
      "      copying gdal-utils\\osgeo_utils\\gdal_retile.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\n",
      "      copying gdal-utils\\osgeo_utils\\gdal_sieve.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\n",
      "      copying gdal-utils\\osgeo_utils\\ogrmerge.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\n",
      "      copying gdal-utils\\osgeo_utils\\ogr_layer_algebra.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\n",
      "      copying gdal-utils\\osgeo_utils\\pct2rgb.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\n",
      "      copying gdal-utils\\osgeo_utils\\rgb2pct.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\n",
      "      copying gdal-utils\\osgeo_utils\\__init__.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\n",
      "      creating build\\lib.win-amd64-cpython-311\\osgeo_utils\\auxiliary\n",
      "      copying gdal-utils\\osgeo_utils\\auxiliary\\array_util.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\auxiliary\n",
      "      copying gdal-utils\\osgeo_utils\\auxiliary\\base.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\auxiliary\n",
      "      copying gdal-utils\\osgeo_utils\\auxiliary\\batch_creator.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\auxiliary\n",
      "      copying gdal-utils\\osgeo_utils\\auxiliary\\color_palette.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\auxiliary\n",
      "      copying gdal-utils\\osgeo_utils\\auxiliary\\color_table.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\auxiliary\n",
      "      copying gdal-utils\\osgeo_utils\\auxiliary\\extent_util.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\auxiliary\n",
      "      copying gdal-utils\\osgeo_utils\\auxiliary\\gdal_argparse.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\auxiliary\n",
      "      copying gdal-utils\\osgeo_utils\\auxiliary\\numpy_util.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\auxiliary\n",
      "      copying gdal-utils\\osgeo_utils\\auxiliary\\osr_util.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\auxiliary\n",
      "      copying gdal-utils\\osgeo_utils\\auxiliary\\progress.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\auxiliary\n",
      "      copying gdal-utils\\osgeo_utils\\auxiliary\\raster_creation.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\auxiliary\n",
      "      copying gdal-utils\\osgeo_utils\\auxiliary\\rectangle.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\auxiliary\n",
      "      copying gdal-utils\\osgeo_utils\\auxiliary\\util.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\auxiliary\n",
      "      copying gdal-utils\\osgeo_utils\\auxiliary\\__init__.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\auxiliary\n",
      "      creating build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\assemblepoly.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\build_jp2_from_xml.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\classify.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\crs2crs2grid.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\densify.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\dump_jp2.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\epsg_tr.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\esri2wkt.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\fft.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\fix_gpkg.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\gcps2ogr.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\gcps2vec.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\gcps2wld.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\gdal2grd.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\gdalchksum.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\gdalcopyproj.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\gdalfilter.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\gdalident.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\gdalimport.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\gdalinfo.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\gdallocationinfo.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\gdal_auth.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\gdal_cp.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\gdal_create_pdf.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\gdal_ls.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\gdal_lut.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\gdal_mkdir.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\gdal_remove_towgs84.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\gdal_rm.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\gdal_rmdir.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\gdal_vrtmerge.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\get_soundg.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\histrep.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\hsv_merge.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\jpeg_in_tiff_extract.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\load2odbc.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\loslas2ntv2.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\magphase.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\make_fuzzer_friendly_archive.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\mkgraticule.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\ogr2ogr.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\ogr2vrt.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\ogrinfo.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\ogrupdate.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\ogr_build_junction_table.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\ogr_dispatch.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\rel.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\tigerpoly.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\tile_extent_from_raster.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\tolatlong.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\validate_cloud_optimized_geotiff.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\validate_geoparquet.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\validate_gpkg.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\validate_jp2.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\val_repl.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\vec_tr.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\vec_tr_spat.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\wcs_virtds_params.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      copying gdal-utils\\osgeo_utils\\samples\\__init__.py -> build\\lib.win-amd64-cpython-311\\osgeo_utils\\samples\n",
      "      running build_ext\n",
      "      building 'osgeo._gdal' extension\n",
      "      building 'osgeo._gnm' extension\n",
      "      building 'osgeo._gdalconst' extension\n",
      "      building 'osgeo._gdal_array' extension\n",
      "      building 'osgeo._ogr' extension\n",
      "      building 'osgeo._osr' extension\n",
      "      creating build\\temp.win-amd64-cpython-311\n",
      "      creating build\\temp.win-amd64-cpython-311\\Release\n",
      "      creating build\\temp.win-amd64-cpython-311\\Release\\extensions\n",
      "      \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.39.33519\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -Ic:\\ProgramData\\anaconda3\\include -Ic:\\ProgramData\\anaconda3\\Include -Ic:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\core\\include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.39.33519\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt\" /EHsc /Tpextensions/ogr_wrap.cpp /Fobuild\\temp.win-amd64-cpython-311\\Release\\extensions/ogr_wrap.obj\n",
      "      ogr_wrap.cpp\n",
      "      \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.39.33519\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -Ic:\\ProgramData\\anaconda3\\include -Ic:\\ProgramData\\anaconda3\\Include -Ic:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\core\\include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.39.33519\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt\" /EHsc /Tpextensions/gdal_wrap.cpp /Fobuild\\temp.win-amd64-cpython-311\\Release\\extensions/gdal_wrap.obj\n",
      "      gdal_wrap.cpp\n",
      "      \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.39.33519\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -Ic:\\ProgramData\\anaconda3\\include -Ic:\\ProgramData\\anaconda3\\Include -Ic:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\core\\include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.39.33519\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt\" /Tcextensions/gdalconst_wrap.c /Fobuild\\temp.win-amd64-cpython-311\\Release\\extensions/gdalconst_wrap.obj\n",
      "      gdalconst_wrap.c\n",
      "      \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.39.33519\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -Ic:\\ProgramData\\anaconda3\\include -Ic:\\ProgramData\\anaconda3\\Include -Ic:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\core\\include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.39.33519\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt\" /EHsc /Tpextensions/gdal_array_wrap.cpp /Fobuild\\temp.win-amd64-cpython-311\\Release\\extensions/gdal_array_wrap.obj\n",
      "      gdal_array_wrap.cpp\n",
      "      extensions/gdalconst_wrap.c(3013): fatal error C1083: Cannot open include file: 'gdal.h': No such file or directory\n",
      "      \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.39.33519\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -Ic:\\ProgramData\\anaconda3\\include -Ic:\\ProgramData\\anaconda3\\Include -Ic:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\core\\include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.39.33519\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt\" /EHsc /Tpextensions/gnm_wrap.cpp /Fobuild\\temp.win-amd64-cpython-311\\Release\\extensions/gnm_wrap.obj\n",
      "      gnm_wrap.cpp\n",
      "      extensions/ogr_wrap.cpp(3181): fatal error C1083: Cannot open include file: 'gdal.h': No such file or directory\n",
      "      extensions/gdal_wrap.cpp(3222): fatal error C1083: Cannot open include file: 'cpl_port.h': No such file or directory\n",
      "      extensions/gdal_array_wrap.cpp(3167): fatal error C1083: Cannot open include file: 'gdal.h': No such file or directory\n",
      "      extensions/gnm_wrap.cpp(3153): fatal error C1083: Cannot open include file: 'gdal.h': No such file or directory\n",
      "      \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.39.33519\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -Ic:\\ProgramData\\anaconda3\\include -Ic:\\ProgramData\\anaconda3\\Include -Ic:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\core\\include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.39.33519\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt\" /EHsc /Tpextensions/osr_wrap.cpp /Fobuild\\temp.win-amd64-cpython-311\\Release\\extensions/osr_wrap.obj\n",
      "      osr_wrap.cpp\n",
      "      extensions/osr_wrap.cpp(3213): fatal error C1083: Cannot open include file: 'cpl_string.h': No such file or directory\n",
      "      error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2022\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for GDAL\n",
      "ERROR: Could not build wheels for GDAL, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "# pip install packages\n",
    "import sys\n",
    "!{sys.executable} -m pip install geopandas\n",
    "!{sys.executable} -m pip install GDAL\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker as mtick\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "\n",
    "# Letting pandas to show max columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row ID</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Ship Date</th>\n",
       "      <th>Ship Mode</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Customer Name</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Postal Code</th>\n",
       "      <th>Region</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7773</td>\n",
       "      <td>CA-2016-108196</td>\n",
       "      <td>25/11/2016</td>\n",
       "      <td>12/02/2016</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>CS-12505</td>\n",
       "      <td>Cindy Stewart</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Lancaster</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>43130</td>\n",
       "      <td>Est</td>\n",
       "      <td>TEC-MA-10000418</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Machines</td>\n",
       "      <td>Cubify CubeX 3D Printer Double Head Print</td>\n",
       "      <td>4499.985</td>\n",
       "      <td>5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-6599.978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>684</td>\n",
       "      <td>US-2017-168116</td>\n",
       "      <td>11/04/2017</td>\n",
       "      <td>11/04/2017</td>\n",
       "      <td>Same Day</td>\n",
       "      <td>GT-14635</td>\n",
       "      <td>Grant Thornton</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>United States</td>\n",
       "      <td>Burlington</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>\"27217\"</td>\n",
       "      <td>South</td>\n",
       "      <td>TEC-MA-10004125</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Machines</td>\n",
       "      <td>Cubify CubeX 3D Printer Triple Head Print</td>\n",
       "      <td>7999.980</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-3839.9904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9775</td>\n",
       "      <td>CA-2014-169019</td>\n",
       "      <td>26/07/2014</td>\n",
       "      <td>30/07/2014</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>LF-17185</td>\n",
       "      <td>Luke Foster</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>Texas</td>\n",
       "      <td>78207</td>\n",
       "      <td>Central</td>\n",
       "      <td>OFF-BI-10004995</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Binders</td>\n",
       "      <td>GBC DocuBind P400 Electric Binding System</td>\n",
       "      <td>2177.584</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-3701.8928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3012</td>\n",
       "      <td>CA-2017-134845</td>\n",
       "      <td>17/04/2017</td>\n",
       "      <td>24/04/2017</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>SR-20425</td>\n",
       "      <td>Sharelle Roach</td>\n",
       "      <td>Home Office</td>\n",
       "      <td>United States</td>\n",
       "      <td>Louisville</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>80027</td>\n",
       "      <td>West</td>\n",
       "      <td>TEC-MA-10000822</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Machines</td>\n",
       "      <td>Lexmark MX611dhe Monochrome Laser Printer</td>\n",
       "      <td>2549.985</td>\n",
       "      <td>5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-3399.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4992</td>\n",
       "      <td>US-2017-122714</td>\n",
       "      <td>12/07/2017</td>\n",
       "      <td>13/12/2017</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>HG-14965</td>\n",
       "      <td>Henry Goldwyn</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>United States</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>60653</td>\n",
       "      <td>Central</td>\n",
       "      <td>OFF-BI-10001120</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Binders</td>\n",
       "      <td>Ibico EPK-21 Electric Binding System</td>\n",
       "      <td>1889.990</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-2929.4845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3152</td>\n",
       "      <td>CA-2015-147830</td>\n",
       "      <td>15/12/2015</td>\n",
       "      <td>18/12/2015</td>\n",
       "      <td>First Class</td>\n",
       "      <td>NF-18385</td>\n",
       "      <td>Natalie Fritzler</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Newark</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>43055</td>\n",
       "      <td>East</td>\n",
       "      <td>TEC-MA-10000418</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Machines</td>\n",
       "      <td>Cubify CubeX 3D Printer Double Head Print</td>\n",
       "      <td>1799.994</td>\n",
       "      <td>Two</td>\n",
       "      <td>0.7</td>\n",
       "      <td>\"-2639.9912\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5311</td>\n",
       "      <td>CA-2017-131254</td>\n",
       "      <td>19/11/2017</td>\n",
       "      <td>21/11/2017</td>\n",
       "      <td>First Class</td>\n",
       "      <td>NC-18415</td>\n",
       "      <td>Nathan Cano</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Texas</td>\n",
       "      <td>77095</td>\n",
       "      <td>Central</td>\n",
       "      <td>OFF-BI-10003527</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Binders</td>\n",
       "      <td>Fellowes PB500 Electric Punch Plastic Comb Bin...</td>\n",
       "      <td>1525.188</td>\n",
       "      <td>6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-2287.782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9640</td>\n",
       "      <td>CA-2015-116638</td>\n",
       "      <td>28/01/2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>JH-15985</td>\n",
       "      <td>Joseph Holt</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Concord</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>28027</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-TA-10000198</td>\n",
       "      <td>Frnture</td>\n",
       "      <td>Tables</td>\n",
       "      <td>Chromcraft Bull-Nose Wood Oval Conference Tabl...</td>\n",
       "      <td>4297.644</td>\n",
       "      <td>Thirteen</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1200</td>\n",
       "      <td>CA-2016-130946</td>\n",
       "      <td>04/08/2016</td>\n",
       "      <td>04/12/2016</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>ZC-21910</td>\n",
       "      <td>Zuschuss Carroll</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Texas</td>\n",
       "      <td>77041</td>\n",
       "      <td>Central</td>\n",
       "      <td>OFF-BI-10004995</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Binders</td>\n",
       "      <td>GBC DocuBind P400 Electric Binding System</td>\n",
       "      <td>1088.792</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-1850.9464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2698</td>\n",
       "      <td>CA-2014-145317</td>\n",
       "      <td>18/03/2014</td>\n",
       "      <td>23/03/2014</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>SM-20320</td>\n",
       "      <td>Sean Miller</td>\n",
       "      <td>Home Office</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jacksonville</td>\n",
       "      <td>Florida</td>\n",
       "      <td>32216</td>\n",
       "      <td>Southh</td>\n",
       "      <td>TEC-MA-10002412</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Machines</td>\n",
       "      <td>Cisco TelePresence System EX90 Videoconferenci...</td>\n",
       "      <td>22638.480</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-1811.0784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Row ID        Order ID  Order Date   Ship Date       Ship Mode Customer ID  \\\n",
       "0    7773  CA-2016-108196  25/11/2016  12/02/2016  Standard Class    CS-12505   \n",
       "1     684  US-2017-168116  11/04/2017  11/04/2017        Same Day    GT-14635   \n",
       "2    9775  CA-2014-169019  26/07/2014  30/07/2014  Standard Class    LF-17185   \n",
       "3    3012  CA-2017-134845  17/04/2017  24/04/2017  Standard Class    SR-20425   \n",
       "4    4992  US-2017-122714  12/07/2017  13/12/2017  Standard Class    HG-14965   \n",
       "5    3152  CA-2015-147830  15/12/2015  18/12/2015     First Class    NF-18385   \n",
       "6    5311  CA-2017-131254  19/11/2017  21/11/2017     First Class    NC-18415   \n",
       "7    9640  CA-2015-116638  28/01/2015         NaN    Second Class    JH-15985   \n",
       "8    1200  CA-2016-130946  04/08/2016  04/12/2016  Standard Class    ZC-21910   \n",
       "9    2698  CA-2014-145317  18/03/2014  23/03/2014  Standard Class    SM-20320   \n",
       "\n",
       "      Customer Name      Segment        Country          City           State  \\\n",
       "0     Cindy Stewart     Consumer  United States     Lancaster            Ohio   \n",
       "1    Grant Thornton    Corporate  United States    Burlington  North Carolina   \n",
       "2       Luke Foster     Consumer  United States   San Antonio           Texas   \n",
       "3    Sharelle Roach  Home Office  United States    Louisville        Colorado   \n",
       "4     Henry Goldwyn    Corporate  United States       Chicago        Illinois   \n",
       "5  Natalie Fritzler     Consumer  United States        Newark            Ohio   \n",
       "6       Nathan Cano     Consumer  United States       Houston           Texas   \n",
       "7       Joseph Holt     Consumer  United States       Concord  North Carolina   \n",
       "8  Zuschuss Carroll     Consumer  United States       Houston           Texas   \n",
       "9       Sean Miller  Home Office            NaN  Jacksonville         Florida   \n",
       "\n",
       "  Postal Code   Region       Product ID         Category Sub-Category  \\\n",
       "0       43130      Est  TEC-MA-10000418       Technology     Machines   \n",
       "1     \"27217\"    South  TEC-MA-10004125       Technology     Machines   \n",
       "2       78207  Central  OFF-BI-10004995  Office Supplies      Binders   \n",
       "3       80027     West  TEC-MA-10000822       Technology     Machines   \n",
       "4       60653  Central  OFF-BI-10001120  Office Supplies      Binders   \n",
       "5       43055     East  TEC-MA-10000418       Technology     Machines   \n",
       "6       77095  Central  OFF-BI-10003527  Office Supplies      Binders   \n",
       "7       28027    South  FUR-TA-10000198          Frnture       Tables   \n",
       "8       77041  Central  OFF-BI-10004995  Office Supplies      Binders   \n",
       "9       32216   Southh  TEC-MA-10002412       Technology     Machines   \n",
       "\n",
       "                                        Product Name      Sales  Quantity  \\\n",
       "0          Cubify CubeX 3D Printer Double Head Print   4499.985         5   \n",
       "1          Cubify CubeX 3D Printer Triple Head Print   7999.980         4   \n",
       "2          GBC DocuBind P400 Electric Binding System   2177.584         8   \n",
       "3          Lexmark MX611dhe Monochrome Laser Printer   2549.985         5   \n",
       "4               Ibico EPK-21 Electric Binding System   1889.990         5   \n",
       "5          Cubify CubeX 3D Printer Double Head Print   1799.994       Two   \n",
       "6  Fellowes PB500 Electric Punch Plastic Comb Bin...   1525.188         6   \n",
       "7  Chromcraft Bull-Nose Wood Oval Conference Tabl...   4297.644  Thirteen   \n",
       "8          GBC DocuBind P400 Electric Binding System   1088.792         4   \n",
       "9  Cisco TelePresence System EX90 Videoconferenci...  22638.480         6   \n",
       "\n",
       "   Discount        Profit  \n",
       "0       0.7     -6599.978  \n",
       "1       0.5    -3839.9904  \n",
       "2       0.8    -3701.8928  \n",
       "3       0.7      -3399.98  \n",
       "4       0.8    -2929.4845  \n",
       "5       0.7  \"-2639.9912\"  \n",
       "6       0.8     -2287.782  \n",
       "7       0.4           NaN  \n",
       "8       0.8    -1850.9464  \n",
       "9       0.5    -1811.0784  "
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading CSV file and assigning into a dataframe ss_data\n",
    "global_super_store_data = pd.read_csv('sample-superstore-2023-T3.csv')\n",
    "\n",
    "# Set the head to 10 to retrieve the first 10 records\n",
    "first_10_rows = global_super_store_data.head(n=10)\n",
    "first_10_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Understanding of the dataset <a id='understanding-dataset'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using info() and describe() function to get the descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9994 entries, 0 to 9993\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Row ID         9994 non-null   int64  \n",
      " 1   Order ID       9993 non-null   object \n",
      " 2   Order Date     9992 non-null   object \n",
      " 3   Ship Date      9991 non-null   object \n",
      " 4   Ship Mode      9990 non-null   object \n",
      " 5   Customer ID    9994 non-null   object \n",
      " 6   Customer Name  9991 non-null   object \n",
      " 7   Segment        9991 non-null   object \n",
      " 8   Country        9990 non-null   object \n",
      " 9   City           9992 non-null   object \n",
      " 10  State          9990 non-null   object \n",
      " 11  Postal Code    9991 non-null   object \n",
      " 12  Region         9991 non-null   object \n",
      " 13  Product ID     9992 non-null   object \n",
      " 14  Category       9992 non-null   object \n",
      " 15  Sub-Category   9990 non-null   object \n",
      " 16  Product Name   9991 non-null   object \n",
      " 17  Sales          9993 non-null   float64\n",
      " 18  Quantity       9989 non-null   object \n",
      " 19  Discount       9991 non-null   float64\n",
      " 20  Profit         9983 non-null   object \n",
      "dtypes: float64(2), int64(1), object(18)\n",
      "memory usage: 1.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row ID</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9994.000000</td>\n",
       "      <td>9993.000000</td>\n",
       "      <td>9991.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4997.500000</td>\n",
       "      <td>229.863780</td>\n",
       "      <td>0.156180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2885.163629</td>\n",
       "      <td>623.276019</td>\n",
       "      <td>0.206399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.444000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2499.250000</td>\n",
       "      <td>17.280000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4997.500000</td>\n",
       "      <td>54.480000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7495.750000</td>\n",
       "      <td>209.940000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9994.000000</td>\n",
       "      <td>22638.480000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Row ID         Sales     Discount\n",
       "count  9994.000000   9993.000000  9991.000000\n",
       "mean   4997.500000    229.863780     0.156180\n",
       "std    2885.163629    623.276019     0.206399\n",
       "min       1.000000      0.444000     0.000000\n",
       "25%    2499.250000     17.280000     0.000000\n",
       "50%    4997.500000     54.480000     0.200000\n",
       "75%    7495.750000    209.940000     0.200000\n",
       "max    9994.000000  22638.480000     0.800000"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the metadata information about the dataset\n",
    "global_super_store_data.info()\n",
    "\n",
    "# Get descriptive statistics on the dataset\n",
    "global_super_store_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary key of these records are a system-generated, and denoted as column: *RowID*\n",
    "\n",
    "The datatypes of the dataset are following:\n",
    "* int64(1)\n",
    "* float64(2)\n",
    "* object(18)\n",
    "\n",
    "A few records of *Quantity* and *Profit* columns has the datatype of object, but it must be float64, thus needs to be cleansed or transformed.  \n",
    "*Ship Date* and *Order Date* columns are represented as strings, those needs to be converted as datetime.\n",
    "\n",
    "Once cleansed, the descriptive statistics can be applied to the numerial columns, and they are Sales, Quantity, Discount and Profit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function **text2float()** will take a txt number as a parameter and convert back to float64 number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2float(textnum, numwords={}):\n",
    "    try:\n",
    "        # Attempt to convert to float\n",
    "        return float(textnum)\n",
    "    except ValueError:\n",
    "        # If conversion to float fails, continue with text to number conversion\n",
    "        textnum = textnum.lower()\n",
    "        \n",
    "        if not numwords:\n",
    "            units = [\n",
    "                \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
    "                \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "                \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\",\n",
    "            ]\n",
    "\n",
    "            tens = [\"\", \"\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"]\n",
    "\n",
    "            scales = [\"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n",
    "\n",
    "            numwords[\"and\"] = (1, 0)\n",
    "            for idx, word in enumerate(units):\n",
    "                numwords[word] = (1, idx)\n",
    "            for idx, word in enumerate(tens):\n",
    "                numwords[word] = (1, idx * 10)\n",
    "            for idx, word in enumerate(scales):\n",
    "                numwords[word] = (10 ** (idx * 3 or 2), 0)\n",
    "\n",
    "        current = result = 0\n",
    "        for word in textnum.split():\n",
    "            if word not in numwords:\n",
    "                raise Exception(\"Illegal word: \" + word)\n",
    "\n",
    "            scale, increment = numwords[word]\n",
    "            current = current * scale + increment\n",
    "            if scale > 100:\n",
    "                result += current\n",
    "                current = 0\n",
    "\n",
    "        return result + current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_from_postal_code(postal_code):\n",
    "    if postal_code == '':\n",
    "        return None\n",
    "\n",
    "    url = f\"http://api.zippopotam.us/us/{postal_code}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        city = data['places'][0]['place name']\n",
    "        return city\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_from_postal_code(postal_code):\n",
    "    if postal_code == '':\n",
    "        return None\n",
    "\n",
    "    url = f\"http://api.zippopotam.us/us/{postal_code}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        state = data['places'][0]['state']\n",
    "        return state\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing values in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing any statistical analysis, the numerical column data has to be cleansed to be meaningful.\n",
    "* Records with special characters on *Quantity* and needs to be cleansed. \n",
    "* Records with special characters on *Profit* and needs to be cleansed. \n",
    "* Applying the **text2float()** function to fix *Quantity* column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row ID is not needed for the analysis, hence dropping the column\n",
    "if 'Row ID' in global_super_store_data.columns:\n",
    "    global_super_store_data.drop('Row ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing \"?\" from Quantity column\n",
    "global_super_store_data['Quantity'] = global_super_store_data['Quantity'].str.replace('?', '')\n",
    "\n",
    "# Removing \"\"\" from Profit column\n",
    "global_super_store_data['Profit'] = global_super_store_data['Profit'].str.replace('\"', '')\n",
    "\n",
    "# Assuming zero values for NaN on Profits\n",
    "global_super_store_data['Profit'] = global_super_store_data['Profit'].fillna(0)\n",
    "\n",
    "# Removing \"\"\" from Postal Code column\n",
    "global_super_store_data['Postal Code'] = global_super_store_data['Postal Code'].str.replace('\"', '')\n",
    "\n",
    "# Cleanse the country column\n",
    "country_data_to_clear = ['', 'US', '56']\n",
    "global_super_store_data['Country'] = global_super_store_data['Country'].str.replace(country_data_to_clear, 'United States')\n",
    "\n",
    "# Correcting spelling mistakes on Category column\n",
    "global_super_store_data['Category'] = global_super_store_data['Category'].replace('Frnture', 'Furniture')\n",
    "\n",
    "# Filling values on empty Category/Sub-Category records\n",
    "global_super_store_data['Category'] = global_super_store_data['Category'].fillna('NO_CATEGORY')\n",
    "global_super_store_data['Sub-Category'] = global_super_store_data['Sub-Category'].fillna('NO_SUB_CATEGORY')\n",
    "\n",
    "# Datafix on Category based on subcategories\n",
    "# Apply the condition element-wise\n",
    "condition = (global_super_store_data['Category'] == 'NO_CATEGORY') & \\\n",
    "            (global_super_store_data['Sub-Category'].isin(['Binders', 'Storage']))\n",
    "\n",
    "# Update 'Category' where the condition is True\n",
    "global_super_store_data.loc[condition, 'Category'] = 'Office Supplies'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanse the Regions\n",
    "central_regions_to_replace = ['Centrl', 'Cntral']\n",
    "east_regions_to_replace = ['Est']\n",
    "south_regions_to_replace = ['Southh']\n",
    "\n",
    "global_super_store_data['Region'] = global_super_store_data['Region'].replace(central_regions_to_replace, 'Central')\n",
    "global_super_store_data['Region'] = global_super_store_data['Region'].replace(east_regions_to_replace, 'East')\n",
    "global_super_store_data['Region'] = global_super_store_data['Region'].replace(south_regions_to_replace, 'South')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying get_state_from_postal_code function\n",
    "\n",
    "# Filter the empty state rows\n",
    "state_filtered_na = global_super_store_data.loc[pd.isna(global_super_store_data['State'])]\n",
    "state_filtered_na\n",
    "\n",
    "# Apply the function to fill the missing value via API\n",
    "global_super_store_data.loc[pd.isna(global_super_store_data['State']), 'State'] = state_filtered_na['Postal Code'].apply(get_state_from_postal_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying get_city_from_postal_code function\n",
    "\n",
    "# Filter the empty city rows\n",
    "city_filtered_na = global_super_store_data.loc[pd.isna(global_super_store_data['City'])]\n",
    "city_filtered_na\n",
    "\n",
    "# Apply the function to fill the missing value via API\n",
    "global_super_store_data.loc[pd.isna(global_super_store_data['City']), 'City'] = city_filtered_na['Postal Code'].apply(get_city_from_postal_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_super_store_data.to_csv('27042024.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying text2float function\n",
    "global_super_store_data['Quantity'] = global_super_store_data['Quantity'].apply(text2float)\n",
    "global_super_store_data['Profit'] = global_super_store_data['Profit'].apply(text2float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'osgeo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[377], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mosgeo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gdal\n\u001b[0;32m      5\u001b[0m gdal\u001b[38;5;241m.\u001b[39mSetConfigOption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSHAPE_RESTORE_SHX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYES\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load the shapefile of the United States\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'osgeo'"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from osgeo import gdal\n",
    "gdal.SetConfigOption('SHAPE_RESTORE_SHX', 'YES')\n",
    "\n",
    "\n",
    "# Load the shapefile of the United States\n",
    "us_map = gpd.read_file('cb_2017_us_state_500k.shp')\n",
    "\n",
    "# Keep only the continental US\n",
    "continental_us = us_map[(us_map['STUSPS'] != 'AK') & (us_map['STUSPS'] != 'HI') & (us_map['STUSPS'] != 'PR')]\n",
    "\n",
    "# Example sales data\n",
    "sales_data = {'State': ['CA', 'TX', 'NY', 'FL'], 'Sales': [1000, 1500, 800, 1200]}\n",
    "\n",
    "# Convert sales data to DataFrame\n",
    "sales_df = pd.DataFrame(sales_data)\n",
    "\n",
    "# Merge sales data with the US map based on state codes or names\n",
    "merged_data = continental_us.merge(sales_df, how='left', left_on='STUSPS', right_on='State')\n",
    "\n",
    "# Plot the map\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 14))\n",
    "continental_us.plot(ax=ax, color='lightgrey', edgecolor='black')\n",
    "merged_data.plot(column='Sales', cmap='Blues', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True, aspect='equal')  # Adjust aspect ratio\n",
    "ax.axis('off')\n",
    "ax.set_title('Sales by US State')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_super_store_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with missing data\n",
    "print(f\"Sum of null records:\\n{global_super_store_data.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Grouping of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping Sales/Profits based on Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_categories = global_super_store_data['Category'].unique()\n",
    "print(unique_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total sales by category from the highest sale.\n",
    "sales_category = global_super_store_data.groupby('Category')['Sales'].sum().sort_values(ascending=False)\n",
    "sales_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total profits by category\n",
    "profit_category = global_super_store_data.groupby('Category')['Profit'].sum().sort_values(ascending=False)\n",
    "profit_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total sales by category, only considering positive sales\n",
    "sales_based_on_category = global_super_store_data.groupby('Category').filter(lambda x: x['Sales'].sum() > 0).groupby('Category')['Sales'].sum()\n",
    "\n",
    "# group total profit by category, only considering positive profits\n",
    "profit_category = global_super_store_data.groupby('Category').filter(lambda x: x['Profit'].sum() > 0).groupby('Category')['Profit'].sum()\n",
    "\n",
    "# figure size\n",
    "plt.figure(figsize=(16,12));\n",
    "\n",
    "# left total sales pie chart\n",
    "plt.subplot(1,2,1); # 1 row, 2 columns, the 1st plot.\n",
    "plt.pie(sales_category.values, labels=sales_category.index, startangle=90, counterclock=False,\n",
    "        autopct=lambda p:f'{p:.1f}% \\n £{p*np.sum(sales_category.values)/100 :,.0f}', \n",
    "        wedgeprops={'linewidth': 1, 'edgecolor':'black', 'alpha':0.75});\n",
    "plt.axis('square');\n",
    "plt.title('Total Sales by Category',  fontdict={'fontsize':16});\n",
    "\n",
    "# right total profits pie chart\n",
    "plt.subplot(1,2,2); # 1 row, 2 columns, the 2nd plot\n",
    "plt.pie(profit_category.values, labels=profit_category.index, startangle=90, counterclock=False,\n",
    "        autopct=lambda p:f'{p:.1f}% \\n ${p*np.sum(profit_category.values)/100 :,.0f}',\n",
    "        wedgeprops={'linewidth': 1, 'edgecolor':'black', 'alpha':0.75});\n",
    "plt.axis('square');\n",
    "plt.title('Total Profit by Category', fontdict={'fontsize':16});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > _Total Sales on Categories_\n",
    "\n",
    "1. Technology at 36.4% \n",
    "2. Furniture at 32.3%\n",
    "3. Office Supplies at 31.3%\n",
    "\n",
    "Sales depict a near-perfect symmmetery on categories, with **Technology** winning with a slight edge.\n",
    "\n",
    " > _Total Profits on Categories_\n",
    "\n",
    "1. Technology at 50.1%\n",
    "2. Office Supplies at 42.7%\n",
    "3. Furniture at 7.2%\n",
    "\n",
    "Profits are largely taken by **Technology** category with *Office Supplies* being the lowest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping Sales/Profits based on Sub-Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sub_categories = global_super_store_data['Sub-Category'].unique()\n",
    "print(unique_sub_categories)\n",
    "\n",
    "print(type(unique_sub_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total sales by sub-category from the highest sale.\n",
    "sales_sub_category = global_super_store_data.groupby(['Sub-Category'], as_index=False)['Sales'].sum().sort_values(by='Sub-Category')\n",
    "print(sales_sub_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group total profit by sub-category from the highest profit.\n",
    "profit_sub_category = global_super_store_data.groupby(['Sub-Category'], as_index=False)['Profit'].sum().sort_values(by='Sub-Category')\n",
    "print(profit_sub_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping the data on category and it's respective sub-categories. Calculating the profit margin.\n",
    "sales_per_cat_subcat = global_super_store_data.groupby(['Category', 'Sub-Category'], as_index=False)[['Sales', 'Profit']].sum()\n",
    "sales_per_cat_subcat['Profit Margin'] = sales_per_cat_subcat['Profit'] / sales_per_cat_subcat['Sales']\n",
    "\n",
    "#Sorting the dataframe based on profit margin\n",
    "sales_per_cat_subcat = sales_per_cat_subcat.sort_values(by=['Category', 'Sub-Category', 'Profit Margin'], ascending=True)\n",
    "sales_per_cat_subcat.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Univariate analysis and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping the data on category and it's respective sub-categories. Calculating the profit margin.\n",
    "sales_per_cat_subcat = global_super_store_data.groupby(['Category', 'Sub-Category'], as_index=False)[['Sales', 'Profit']].sum()\n",
    "sales_per_cat_subcat['Profit Margin'] = sales_per_cat_subcat['Profit'] / sales_per_cat_subcat['Sales']\n",
    "\n",
    "#Sorting the dataframe based on profit margin\n",
    "sales_per_cat_subcat = sales_per_cat_subcat.sort_values(by=['Category', 'Sub-Category', 'Profit Margin'], ascending=True)\n",
    "\n",
    "# plot a profit margins sub-category bar chart \n",
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "\n",
    "# Unique sub categories without NO_SUB_CATEGORY\n",
    "unique_sub_categories_without_custom_label = sales_per_cat_subcat[sales_per_cat_subcat['Sub-Category'] != 'NO_SUB_CATEGORY']['Sub-Category'].unique()\n",
    "\n",
    "#Plotting the profit margin per sub-category.\n",
    "sns.barplot(y=sales_per_cat_subcat['Sub-Category'], x=sales_per_cat_subcat['Profit Margin'], hue=sales_per_cat_subcat['Category'], \n",
    "                alpha=1, dodge=False, ax=ax, order=unique_sub_categories_without_custom_label)\n",
    "\n",
    "#Cleaning out bar junk\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines[['right','top']].set_visible(False)\n",
    "ax.set(ylabel=None, xlabel='Profit Margin (%)')\n",
    "\n",
    "def move_ylabel_tick(index: list):\n",
    "    \"\"\"\n",
    "    Moving the provided ylabel ticks\n",
    "    \"\"\"\n",
    "    for tick in index:\n",
    "        ax.get_yticklabels()[tick].set_x(0.02)\n",
    "        ax.get_yticklabels()[tick].set_horizontalalignment('right')\n",
    "\n",
    "#Moving the y-labels on sub-categories that are making a loss in order to prevent collision of the bar and the text.\n",
    "move_ylabel_tick([-1, -2, -3])\n",
    "\n",
    "#Annotating the profit margin amount for each bar.\n",
    "for p in ax.patches:\n",
    "    _, y = p.get_xy()\n",
    "    \n",
    "    ax.annotate(f'{p.get_width()*100 :.1f}%', (p.get_width() / 2, y + 0.45))\n",
    "    \n",
    "#Calculating Superstore's aggregate profit margin in order to compare it to each sub-category's profit margin\n",
    "mean_profit = sales_per_cat_subcat['Profit'].sum() / sales_per_cat_subcat['Sales'].sum()\n",
    "\n",
    "#Plotting a vertical line and annotating the Superstore's aggregate profit margin.\n",
    "ax.axvline(mean_profit, color='blue', label='Mean Profit, All Categories', alpha=0.75, ls='-.')\n",
    "\n",
    "#Setting the title and legend.\n",
    "ax.set_title('Profit Margin by Sub-Category', fontdict={'fontsize':16})\n",
    "ax.legend(loc=(1, 0.9))\n",
    "\n",
    "#Formatting the x-axis as %\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = global_super_store_numerial_data.corr()\n",
    "correlation_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot correlation matrix as heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
